{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cordless-wisdom",
   "metadata": {},
   "source": [
    "# XGBoost Regression Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-jungle",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-james",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from glob import glob\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, plotting\n",
    "from hyperopt.pyll.base import scope\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-journalism",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%aimport src.ml_custom_transformers\n",
    "import src.ml_custom_transformers as ct\n",
    "\n",
    "%aimport src.xgboost_helpers\n",
    "from src.xgboost_helpers import get_xgboost_training_curves\n",
    "\n",
    "%aimport src.metrics_helpers\n",
    "from src.metrics_helpers import rmspe_xg, rmspe, rmse\n",
    "\n",
    "%aimport src.utils\n",
    "from src.utils import get_xgboost_preds_obs\n",
    "\n",
    "%aimport src.data_helpers\n",
    "from src.data_helpers import sample_train_val_split, split_data\n",
    "\n",
    "%aimport src.test_helpers\n",
    "from src.test_helpers import (\n",
    "    test_dfgetxy,\n",
    "    test_split_data,\n",
    "    test_fillna,\n",
    "    test_sampled_data_sizes,\n",
    "    test_target_encode_categorical_features,\n",
    "    test_log1p,\n",
    ")\n",
    "\n",
    "%aimport src.hyperopt_helpers\n",
    "from src.hyperopt_helpers import summarize_hyperopt_trials\n",
    "\n",
    "%aimport src.visualization_helpers\n",
    "from src.visualization_helpers import plot_qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-organization",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0.  [About](#about)\n",
    "1.  [User Inputs](#user-inputs)\n",
    "2.  [Load Data](#load-data)\n",
    "3.  [Preprocessing - Set index for Data](#preprocessing---set-index-for-data)\n",
    "4.  [Create Training, Validation and Testing Splits from Training Data](#create-training-validation-and-testing-splits-from-training-data)\n",
    "5.  [Show `datatype`s and Missing Values](#show-`datatype`s-and-missing-values)\n",
    "    -   5.1. [Inspect Columns with Missing Values](#inspect-columns-with-missing-values)\n",
    "6.  [Hyper-Parameter Tuning with K-Fold Cross-Validation](#hyper-parameter-tuning-with-k-fold-cross-validation)\n",
    "    -   6.1. [Check Data Processing during Cross-Validation](#check-data-processing-during-cross-validation)\n",
    "    -   6.2. [Tune HyperParameters](#tune-hyperparameters)\n",
    "    -   6.3. [Get Best HyperParameters](#get-best-hyperparameters)\n",
    "7. [Extract Features and Target from all Splits](#extract-features-and-target-from-all-splits)\n",
    "8. [Handle missing values in each split - Use Median of Training Data](#handle-missing-values-in-each-split---use-median-of-training-data)\n",
    "9.  [Encode Categorical Features](#encode-categorical-features)\n",
    "10.  [Scaling Numerical Features](#scaling-numerical-features)\n",
    "11.  [Transform Target](#transform-target)\n",
    "12.  [Train-Predict with Best HyperParameters](#train-predict-with-best-hyperparameters)\n",
    "     -   12.1. [Train with best hyper-parameters](#train-with-best-hyper-parameters)\n",
    "     -   12.2. [XGBoost Training Curves - Training and Validation Splits](#xgboost-training-curves---training-and-validation-splits)\n",
    "     -   12.3. [Make Predictions](#make-predictions)\n",
    "13.  [Model Evaluation on Test Split](#model-evaluation-on-test-split)\n",
    "     -   13.1. [Evaluation Metric](#evaluation-metric)\n",
    "     -   13.2. [Distribution of Residuals](#distribution-of-residuals)\n",
    "     -   13.3. [Prediction Error - Observed vs Predicted](#prediction-error---observed-vs-predicted)\n",
    "     -   13.4. [Residuals vs Observed](#residuals-vs-observed)\n",
    "     -   13.5. [Quantile-Quantile Plot for Residuals](#quantile-quantile-plot-for-residuals)\n",
    "14.  [Export trained ML model](#export-trained-ml-model)\n",
    "     -   16.1. [Train ML model with all available training data and no early-stopping](#train-ml-model-with-all-available-training-data-and-no-early-stopping)\n",
    "     -   16.2. [Export ML model to file](#export-ml-model-to-file)\n",
    "15. [Future Work](#future-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-mission",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-17T14:26:32.381936Z",
     "iopub.status.busy": "2021-03-17T14:26:32.381752Z",
     "iopub.status.idle": "2021-03-17T14:26:32.387662Z",
     "shell.execute_reply": "2021-03-17T14:26:32.387012Z",
     "shell.execute_reply.started": "2021-03-17T14:26:32.381889Z"
    }
   },
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-cross",
   "metadata": {},
   "source": [
    "In this notebook, we'll run ML regression trials using XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-refund",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-trick",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJ_ROOT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-bobby",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"Store\",\n",
    "    \"DayOfWeek\",\n",
    "    \"Year\",\n",
    "    \"Month\",\n",
    "    \"Day\",\n",
    "    \"StateHoliday\",\n",
    "    \"CompetitionMonthsOpen\",\n",
    "    \"Promo2Weeks\",\n",
    "    \"StoreType\",\n",
    "    \"Assortment\",\n",
    "    \"PromoInterval\",\n",
    "    \"CompetitionOpenSinceYear\",\n",
    "    \"Promo2SinceYear\",\n",
    "    \"State\",\n",
    "    \"Week\",\n",
    "    \"Events\",\n",
    "    \"Promo_fw\",\n",
    "    \"Promo_bw\",\n",
    "    \"StateHoliday_fw\",\n",
    "    \"StateHoliday_bw\",\n",
    "    \"SchoolHoliday_fw\",\n",
    "    \"SchoolHoliday_bw\",\n",
    "    \"CompetitionDistance\",\n",
    "    \"Max_TemperatureC\",\n",
    "    \"Mean_TemperatureC\",\n",
    "    \"Min_TemperatureC\",\n",
    "    \"Max_Humidity\",\n",
    "    \"Mean_Humidity\",\n",
    "    \"Min_Humidity\",\n",
    "    \"Max_Wind_SpeedKm_h\",\n",
    "    \"Mean_Wind_SpeedKm_h\",\n",
    "    \"CloudCover\",\n",
    "    # \"trend\",\n",
    "    # \"trend_DE\",\n",
    "    \"AfterStateHoliday\",\n",
    "    \"BeforeStateHoliday\",\n",
    "    \"Promo\",\n",
    "    \"SchoolHoliday\",\n",
    "    \"Date\",\n",
    "    \"Sales\",\n",
    "]\n",
    "continuous_vars_missing_vals = [\"CompetitionDistance\", \"CloudCover\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-short",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_data_path = os.path.join(PROJ_ROOT_DIR, \"data\", \"processed\")\n",
    "train_parquet_filepath = os.path.join(\n",
    "    processed_data_path, \"cleaned_train\" + \"_*\" + \".parquet.gzip\"\n",
    ")\n",
    "holdout_parquet_filepath = os.path.join(\n",
    "    processed_data_path, \"cleaned_test\" + \"_*\" + \".parquet.gzip\"\n",
    ")\n",
    "train_parquet_full_filepath = glob(train_parquet_filepath)[0]\n",
    "print(train_parquet_full_filepath)\n",
    "holdout_parquet_full_filepath = glob(holdout_parquet_filepath)[0]\n",
    "print(holdout_parquet_full_filepath)\n",
    "\n",
    "seed = 123\n",
    "space = {\n",
    "    # Learning rate: default 0.3 -> range: [0,3]\n",
    "    \"eta\": hp.quniform(\"eta\", 0.01, 0.3, 0.001),\n",
    "    # Control complexity (control overfitting)\n",
    "    # Maximum depth of a tree: default 6 -> range: [0:∞]\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 5, 10, 1)),\n",
    "    # Minimum sum of instance weight (hessian) needed in a child: default 1\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 3, 1),\n",
    "    # Minimum loss reduction required: default 0 -> range: [0,∞]\n",
    "    \"gamma\": hp.quniform(\"gamma\", 0, 5, 0.5),\n",
    "    # Add randomness to make training robust to noise (control overfitting)\n",
    "    # Subsample ratio of the training instance: default 1\n",
    "    \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.05),\n",
    "    # Subsample ratio of columns when constructing each tree: default 1\n",
    "    \"colsample_bytree\": hp.quniform(\"colsample_bytree\", 0.5, 1, 0.05),\n",
    "    # Regression problem\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    # For reproducibility\n",
    "    \"seed\": seed,\n",
    "    # Faster computation = gpu_hist\n",
    "    \"tree_method\": \"hist\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-engineering",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-albany",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_score():\n",
    "    _ = score(space, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-dominant",
   "metadata": {},
   "source": [
    "<a id=\"load-data\"></a>\n",
    "\n",
    "## 2. [Load Data](#load-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-cookbook",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 1) Load data\n",
    "tunning_dataset = pd.read_parquet(train_parquet_full_filepath, columns=columns)\n",
    "holdout_dataset = pd.read_parquet(\n",
    "    holdout_parquet_full_filepath, columns=list(set(columns) - (set([\"Sales\"])))\n",
    ")\n",
    "print(tunning_dataset.shape)\n",
    "print(holdout_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-replication",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing---set-index-for-data\"></a>\n",
    "\n",
    "## 3. [Preprocessing - Set index for Data](#preprocessing---set-index-for-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-animation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 2) Let's use the date as the index and sort the data\n",
    "# tunning_dataset.sort_values(\"Date\", inplace=True)\n",
    "# tunning_dataset.set_index(\"Date\", inplace=True)\n",
    "# columns.remove(\"Date\")\n",
    "\n",
    "# tunning_dataset_X = tunning_dataset\n",
    "# tunning_dataset_y = tunning_dataset_X.pop(\"Sales\")\n",
    "\n",
    "# # 2) Let's use the date as the index and sort the data\n",
    "# holdout_dataset.sort_values(\"Date\", inplace=True)\n",
    "# holdout_dataset.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# holdout_dataset_X = holdout_dataset\n",
    "# print(tunning_dataset.shape, tunning_dataset_X.shape, tunning_dataset_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-writer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "setup_pipe = Pipeline(\n",
    "    [\n",
    "        (\"datesort\", ct.DFSortByDateSetDateIndex(\"Date\")),\n",
    "    ]\n",
    ")\n",
    "tunning_dataset = setup_pipe.fit_transform(tunning_dataset)\n",
    "print(tunning_dataset.shape)\n",
    "holdout_dataset = setup_pipe.fit_transform(holdout_dataset)\n",
    "print(holdout_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-inspiration",
   "metadata": {},
   "source": [
    "<a id=\"create-training-validation-and-testing-splits-from-training-data\"></a>\n",
    "\n",
    "## 4. [Create Training, Validation and Testing Splits from Training Data](#create-training-validation-and-testing-splits-from-training-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-contractor",
   "metadata": {},
   "source": [
    "Although a holdout set is provided, the target value is not known there. So, we'll divide the overall training data into training, validation and testing splits, from the training data, in order to assist in model assessment.\n",
    "\n",
    "Hyper-parameter tuning will be preformed using the training split, while the validation and testing split will only be used for assessment (with the best hyper-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-studio",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# n = len(holdout_dataset)\n",
    "# idx = np.arange(0, len(tunning_dataset))\n",
    "# idx.sort()\n",
    "# train_idx_end = len(tunning_dataset) - (2 * n)\n",
    "# val_idx_end = len(tunning_dataset) - (1 * n)\n",
    "# train_index = idx[:train_idx_end]\n",
    "# val_index = idx[train_idx_end:val_idx_end]\n",
    "# test_index = idx[val_idx_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_train, df_val, df_test = (\n",
    "#     tunning_dataset.iloc[train_index].copy(),\n",
    "#     tunning_dataset.iloc[val_index].copy(),\n",
    "#     tunning_dataset.iloc[test_index].copy(),\n",
    "# )\n",
    "# print(len(df_train), len(df_val), len(df_test))\n",
    "# assert len(df_val) == len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-efficiency",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "d = split_data(tunning_dataset, holdout_dataset)\n",
    "train_index, val_index, test_index = d[\"indexes\"]\n",
    "df_train, df_val, df_test = d[\"splits\"]\n",
    "test_split_data(\n",
    "    max(train_index),\n",
    "    max(val_index),\n",
    "    max(test_index),\n",
    "    df_train,\n",
    "    df_val,\n",
    "    df_test,\n",
    "    holdout_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-directory",
   "metadata": {},
   "source": [
    "<a id=\"show-`datatype`s-and-missing-values\"></a>\n",
    "\n",
    "## 5. [Show `datatype`s and Missing Values](#show-`datatype`s-and-missing-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-christian",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_summary = (\n",
    "    df_train.isna()\n",
    "    .sum()\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"NaNs\"})\n",
    "    .merge(\n",
    "        df_train.dtypes.to_frame().rename(columns={0: \"dtype\"}),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    ")\n",
    "display(df_train.head())\n",
    "display(df_train_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-diabetes",
   "metadata": {},
   "source": [
    "<a id=\"inspect-columns-with-missing-values\"></a>\n",
    "\n",
    "### 5.1. [Inspect Columns with Missing Values](#inspect-columns-with-missing-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-proportion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_with_nans = df_train[df_train.columns[(df_train.isna().sum() > 0).tolist()]]\n",
    "display(df_train_with_nans.dtypes.to_frame().T)\n",
    "display(df_train_with_nans.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-poultry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 3) Apply log transform\n",
    "# tunning_dataset_y = np.log1p(tunning_dataset_y)\n",
    "\n",
    "# # Number of cross-validation folds (from the last)\n",
    "# pred_folds = 3\n",
    "# train_times = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-complexity",
   "metadata": {},
   "source": [
    "<a id=\"hyper-parameter-tuning-with-k-fold-cross-validation\"></a>\n",
    "\n",
    "## 6. [Hyper-Parameter Tuning with K-Fold Cross-Validation](#hyper-parameter-tuning-with-k-fold-cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-toronto",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score(params, check_data_run=False):\n",
    "    n_split = 10\n",
    "    train_sample_size = 5_000\n",
    "    val_sample_size = 2_500\n",
    "    rmspe_test_fold_scores = []\n",
    "    tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "    for cv_fold_idx, (train_index, test_index) in enumerate(tscv.split(df_train)):\n",
    "        assert cv_fold_idx + 1 <= n_split\n",
    "        assert train_sample_size <= len(test_index)\n",
    "\n",
    "        # 5) Select a random sample for early stopping\n",
    "        (\n",
    "            train_train_index,\n",
    "            train_es_index,\n",
    "            test_index_sampled,\n",
    "            train_full_index,\n",
    "        ) = sample_train_val_split(\n",
    "            len(train_index), len(test_index), train_sample_size, val_sample_size\n",
    "        )\n",
    "        df_train_cv = df_train.iloc[train_train_index]\n",
    "        df_val_cv = df_train.iloc[train_es_index]\n",
    "        df_test_cv = df_train.iloc[test_index_sampled]\n",
    "        if check_data_run:\n",
    "            print(\n",
    "                f\"k={cv_fold_idx}, \"\n",
    "                f\"Train={len(train_index)}, \"\n",
    "                f\"Train_Sampled={len(df_train_cv)}, \"\n",
    "                f\"Val={len(train_index)}, \"\n",
    "                f\"Val_Sampled={len(df_train_cv)}, \"\n",
    "                f\"Test={len(test_index)}, \"\n",
    "                f\"TestSampled={len(df_test_cv)}\"\n",
    "            )\n",
    "        # print(min(train_train_index), max(train_train_index))\n",
    "        # print(min(train_es_index), max(train_es_index))\n",
    "        # print(min(test_index_sampled), max(test_index_sampled))\n",
    "        test_sampled_data_sizes(\n",
    "            train_train_index,\n",
    "            train_es_index,\n",
    "            test_index_sampled,\n",
    "            val_sample_size,\n",
    "            train_full_index,\n",
    "            df_train_cv,\n",
    "            df_val_cv,\n",
    "            df_test_cv,\n",
    "        )\n",
    "\n",
    "        # 4) Select data by index from the time series cross validatation split\n",
    "        # X_train, X_val, X_test, y_train, y_val, y_test = (\n",
    "        #     tunning_dataset_X.iloc[train_train_index].copy(),\n",
    "        #     tunning_dataset_X.iloc[train_es_index].copy(),\n",
    "        #     tunning_dataset_X.iloc[test_index_sampled].copy(),\n",
    "        #     tunning_dataset_y.iloc[train_train_index].copy(),\n",
    "        #     tunning_dataset_y.iloc[train_es_index].copy(),\n",
    "        #     tunning_dataset_y.iloc[test_index_sampled].copy(),\n",
    "        # )\n",
    "        getxy_pipe = Pipeline(\n",
    "            [\n",
    "                (\"getxy\", ct.DFGetXY(\"Sales\")),\n",
    "            ]\n",
    "        )\n",
    "        X_train, y_train = getxy_pipe.fit_transform(df_train_cv.copy())\n",
    "        X_val, y_val = getxy_pipe.fit_transform(df_val_cv.copy())\n",
    "        X_test, y_test = getxy_pipe.fit_transform(df_test_cv.copy())\n",
    "        test_dfgetxy(X_train, y_train, \"Sales\")\n",
    "        test_dfgetxy(X_val, y_val, \"Sales\")\n",
    "        test_dfgetxy(X_test, y_test, \"Sales\")\n",
    "\n",
    "        # 5) Deal with missing continuous values\n",
    "        # for col_name in continuous_vars_missing_vals:\n",
    "        #     # Add na cols\n",
    "        #     X_train[col_name + \"_na\"] = pd.isnull(X_train[col_name])\n",
    "        #     X_val[col_name + \"_na\"] = pd.isnull(X_val[col_name])\n",
    "        #     X_test[col_name + \"_na\"] = pd.isnull(X_test[col_name])\n",
    "        #     # Fill missing with median (default in FastAI)\n",
    "        #     fillter = X_train[col_name].median()\n",
    "        #     X_train[col_name] = X_train[col_name].fillna(fillter)\n",
    "        #     X_val[col_name] = X_val[col_name].fillna(fillter)\n",
    "        #     X_test[col_name] = X_test[col_name].fillna(fillter)\n",
    "        fillna_pipe = Pipeline(\n",
    "            [\n",
    "                (\"cd\", ct.DFFillNa(\"CompetitionDistance\")),\n",
    "                (\"cda\", ct.DFAddNaIndicator(\"CompetitionDistance\")),\n",
    "                (\"cc\", ct.DFFillNa(\"CloudCover\")),\n",
    "                (\"cca\", ct.DFAddNaIndicator(\"CloudCover\")),\n",
    "            ]\n",
    "        )\n",
    "        fillna_pipe.fit(X_train)\n",
    "        X_train = fillna_pipe.transform(X_train)\n",
    "        X_val = fillna_pipe.transform(X_val)\n",
    "        X_test = fillna_pipe.transform(X_test)\n",
    "        # assert (X_train[continuous_vars_missing_vals].isna().sum().tolist()) == [\n",
    "        #     0\n",
    "        # ] * len(continuous_vars_missing_vals)\n",
    "        test_fillna(X_train, X_val, X_test, continuous_vars_missing_vals)\n",
    "\n",
    "        # 6) Deal with categorical variables\n",
    "        # cat_vars = list(X_train.select_dtypes(include=\"object\"))\n",
    "        # te = TargetEncoder(handle_missing=\"value\")\n",
    "        # X_train = te.fit_transform(X_train, cols=cat_vars, y=y_train)\n",
    "        # X_val = te.transform(X_val)\n",
    "        # X_test = te.transform(X_test)\n",
    "        cat_vars = list(X_train.select_dtypes(include=\"object\"))\n",
    "        cat_pipe = Pipeline(\n",
    "            [\n",
    "                (\"targetcats\", ct.DFTargetEncodeCategoricalFeatures(cat_vars)),\n",
    "            ]\n",
    "        )\n",
    "        cat_pipe.fit(X_train, y_train)\n",
    "        X_train = cat_pipe.transform(X_train)\n",
    "        X_val = cat_pipe.transform(X_val)\n",
    "        X_test = cat_pipe.transform(X_test)\n",
    "        # assert not list(X_train.select_dtypes(include=\"object\"))\n",
    "        # assert not list(X_val.select_dtypes(include=\"object\"))\n",
    "        # assert not list(X_test.select_dtypes(include=\"object\"))\n",
    "        test_target_encode_categorical_features(X_train, X_val, X_test)\n",
    "\n",
    "        # 3) Apply log transform\n",
    "        # y_train = np.log1p(y_train)\n",
    "        # y_val = np.log1p(y_val)\n",
    "        # y_test = np.log1p(y_test)\n",
    "        log1p_pipe = Pipeline(\n",
    "            [\n",
    "                (\"log1p\", ct.DFLog1p(\"Sales\")),\n",
    "            ]\n",
    "        )\n",
    "        y_train = log1p_pipe.fit_transform(y_train.to_frame()).squeeze()\n",
    "        y_val = log1p_pipe.fit_transform(y_val.to_frame()).squeeze()\n",
    "        y_test = log1p_pipe.fit_transform(y_test.to_frame()).squeeze()\n",
    "        test_log1p([y_train, y_val, y_test])\n",
    "\n",
    "        # 7) Convert to DMatrix for XGBoost\n",
    "        dtrain = xgb.DMatrix(X_train, y_train)\n",
    "        dvalid = xgb.DMatrix(X_val, y_val)\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "        watchlist = [(dtrain, \"train\"), (dvalid, \"eval\")]\n",
    "\n",
    "        if not check_data_run:\n",
    "            # Can use feval for a custom objective function\n",
    "            model = xgb.train(\n",
    "                params,\n",
    "                dtrain,\n",
    "                early_stopping_rounds=50,\n",
    "                num_boost_round=400,\n",
    "                verbose_eval=False,\n",
    "                feval=rmspe_xg,\n",
    "                evals=watchlist,\n",
    "            )\n",
    "            # validation - this will be the score that we append to a list,\n",
    "            # which will be fed as the score? Is all of this the score?\n",
    "            y_pred = model.predict(dtest)\n",
    "            rmspe_test = rmspe(y_test, y_pred)\n",
    "\n",
    "            rmspe_test_fold_scores.append(rmspe_test)\n",
    "\n",
    "    if not check_data_run:\n",
    "        return np.mean(rmspe_test_fold_scores)\n",
    "    return [X_train, X_val, X_test, y_train, y_val, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-dublin",
   "metadata": {},
   "source": [
    "<a id=\"check-data-processing-during-cross-validation\"></a>\n",
    "\n",
    "### 6.1. [Check Data Processing during Cross-Validation](#check-data-processing-during-cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-relative",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "check_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-louisiana",
   "metadata": {},
   "source": [
    "<a id=\"tune-hyperparameters\"></a>\n",
    "\n",
    "### 6.2. [Tune HyperParameters](#tune-hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-wright",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# trials will contain logging information\n",
    "trials = Trials()\n",
    "\n",
    "# tune hyper-parameters\n",
    "best_hyperparams = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=100)\n",
    "print(f\"Best hyper-parameters: {best_hyperparams}\")\n",
    "display(summarize_hyperopt_trials(trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-minutes",
   "metadata": {},
   "source": [
    "<a id=\"get-best-hyperparameters\"></a>\n",
    "\n",
    "### 6.3. [Get Best HyperParameters](#get-best-hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-atmosphere",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_hyperparams = {}\n",
    "best_hyperparams[\"objective\"] = \"reg:squarederror\"\n",
    "best_hyperparams[\"seed\"] = seed\n",
    "best_hyperparams[\"tree_method\"] = \"hist\"\n",
    "best_hyperparams[\"max_depth\"] = int(best_hyperparams[\"max_depth\"])\n",
    "# best_hyperparams[\"max_depth\"] = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-exhaust",
   "metadata": {},
   "source": [
    "<a id=\"extract-features-and-target-from-all-splits\"></a>\n",
    "\n",
    "## 7. [Extract Features and Target from all Splits](#extract-features-and-target-from-all-splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-operator",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# X_train, X_val, X_test = (\n",
    "#     tunning_dataset_X.iloc[train_index].copy(),\n",
    "#     tunning_dataset_X.iloc[val_index].copy(),\n",
    "#     tunning_dataset_X.iloc[test_index].copy(),\n",
    "# )\n",
    "# y_train, y_val, y_test = (\n",
    "#     tunning_dataset_y.iloc[train_index].copy(),\n",
    "#     tunning_dataset_y.iloc[val_index].copy(),\n",
    "#     tunning_dataset_y.iloc[test_index].copy(),\n",
    "# )\n",
    "# print(\n",
    "#     X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-senator",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "getxy_pipe = Pipeline(\n",
    "    [\n",
    "        (\"getxy\", ct.DFGetXY(\"Sales\")),\n",
    "    ]\n",
    ")\n",
    "X_train, y_train = getxy_pipe.fit_transform(df_train.copy())\n",
    "X_val, y_val = getxy_pipe.fit_transform(df_val.copy())\n",
    "X_test, y_test = getxy_pipe.fit_transform(df_test.copy())\n",
    "print(\n",
    "    X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-swiss",
   "metadata": {},
   "source": [
    "<a id=\"handle-missing-values-in-each-split---use-median-of-training-data\"></a>\n",
    "\n",
    "## 8. [Handle missing values in each split - Use Median of Training Data](#handle-missing-values-in-each-split---use-median-of-training-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-hamilton",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for col_name in [\"CompetitionDistance\", \"CloudCover\"]:\n",
    "#     # Add na cols\n",
    "#     X_train[col_name + \"_na\"] = pd.isnull(X_train[col_name])\n",
    "#     X_val[col_name + \"_na\"] = pd.isnull(X_val[col_name])\n",
    "#     X_test[col_name + \"_na\"] = pd.isnull(X_test[col_name])\n",
    "#     # Fill missing with median (default in FastAI) of TRAINING set\n",
    "#     fillter = X_train[col_name].median()\n",
    "#     X_train[col_name] = X_train[col_name].fillna(fillter)\n",
    "#     X_val[col_name] = X_val[col_name].fillna(fillter)\n",
    "#     X_test[col_name] = X_test[col_name].fillna(fillter)\n",
    "# assert (X_train[continuous_vars_missing_vals].isna().sum().tolist()) == [0] * len(\n",
    "#     continuous_vars_missing_vals\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-sandwich",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fillna_pipe = Pipeline(\n",
    "    [\n",
    "        (\"cd\", ct.DFFillNa(\"CompetitionDistance\")),\n",
    "        (\"cda\", ct.DFAddNaIndicator(\"CompetitionDistance\")),\n",
    "        (\"cc\", ct.DFFillNa(\"CloudCover\")),\n",
    "        (\"cca\", ct.DFAddNaIndicator(\"CloudCover\")),\n",
    "    ]\n",
    ")\n",
    "fillna_pipe.fit(X_train)\n",
    "X_train = fillna_pipe.transform(X_train)\n",
    "X_val = fillna_pipe.transform(X_val)\n",
    "X_test = fillna_pipe.transform(X_test)\n",
    "test_fillna(X_train, X_val, X_test, continuous_vars_missing_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-assembly",
   "metadata": {},
   "source": [
    "<a id=\"encode-categorical-features\"></a>\n",
    "\n",
    "## 9. [Encode Categorical Features](#encode-categorical-features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-cache",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# cat_vars = list(X_train.select_dtypes(include=\"object\"))\n",
    "# te = TargetEncoder(handle_missing=\"value\")\n",
    "# X_train = te.fit_transform(X_train, cols=cat_vars, y=y_train)\n",
    "# X_val = te.transform(X_val)\n",
    "# X_test = te.transform(X_test)\n",
    "# assert not list(X_train.select_dtypes(include=\"object\"))\n",
    "# assert not list(X_val.select_dtypes(include=\"object\"))\n",
    "# assert not list(X_test.select_dtypes(include=\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-million",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cat_vars = list(X_train.select_dtypes(include=\"object\"))\n",
    "cat_pipe = Pipeline(\n",
    "    [\n",
    "        (\"targetcats\", ct.DFTargetEncodeCategoricalFeatures(cat_vars)),\n",
    "    ]\n",
    ")\n",
    "cat_pipe.fit(X_train, y_train)\n",
    "X_train = cat_pipe.transform(X_train)\n",
    "X_val = cat_pipe.transform(X_val)\n",
    "X_test = cat_pipe.transform(X_test)\n",
    "test_target_encode_categorical_features(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-perception",
   "metadata": {},
   "source": [
    "<a id=\"scaling-numerical-features\"></a>\n",
    "\n",
    "## 10. [Scaling Numerical Features](#scaling-numerical-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-massachusetts",
   "metadata": {},
   "source": [
    "<span style='color:red'><b>(IMPORTANT) To be done</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-therapist",
   "metadata": {},
   "source": [
    "<a id=\"transform-target\"></a>\n",
    "\n",
    "## 11. [Transform Target](#transform-target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-hospital",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_train = np.log1p(y_train)\n",
    "# y_val = np.log1p(y_val)\n",
    "# y_test = np.log1p(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-mississippi",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "log1p_pipe = Pipeline(\n",
    "    [\n",
    "        (\"log1p\", ct.DFLog1p(\"Sales\")),\n",
    "    ]\n",
    ")\n",
    "y_train = log1p_pipe.fit_transform(y_train.to_frame()).squeeze()\n",
    "y_val = log1p_pipe.fit_transform(y_val.to_frame()).squeeze()\n",
    "y_test = log1p_pipe.fit_transform(y_test.to_frame()).squeeze()\n",
    "test_log1p([y_train, y_val, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-adoption",
   "metadata": {},
   "source": [
    "<a id=\"train-predict-with-best-hyperparameters\"></a>\n",
    "\n",
    "## 12. [Train-Predict with Best HyperParameters](#train-predict-with-best-hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-inspector",
   "metadata": {},
   "source": [
    "Convert overall data splits to XGBoost `DMatrix` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-winning",
   "metadata": {},
   "source": [
    "<a id=\"train-with-best-hyper-parameters\"></a>\n",
    "\n",
    "### 12.1. [Train with best hyper-parameters](#train-with-best-hyper-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-climb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "training_curves_002 = {}\n",
    "model_exp002 = xgb.train(\n",
    "    params=best_hyperparams, \n",
    "    dtrain=dtrain, \n",
    "    num_boost_round=400, \n",
    "    early_stopping_rounds=50, \n",
    "    feval=rmspe_xg, \n",
    "    evals=watchlist, \n",
    "    evals_result=training_curves_002\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-output",
   "metadata": {},
   "source": [
    "<a id=\"xgboost-training-curves---training-and-validation-splits\"></a>\n",
    "\n",
    "### 12.2. [XGBoost Training Curves - Training and Validation Splits](#xgboost-training-curves---training-and-validation-splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-edward",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "df_training_curves = get_xgboost_training_curves(training_curves_002, \"rmspe\")\n",
    "df_training_curves.plot(ax=ax)\n",
    "ax.grid()\n",
    "ax.legend(ncol=2, bbox_to_anchor=(0.8, 1.1), loc=\"upper left\", frameon=False)\n",
    "ax.set_title(\"RMSPE Loss vs Iterations\", loc=\"left\", fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-citizenship",
   "metadata": {},
   "source": [
    "<a id=\"make-predictions\"></a>\n",
    "\n",
    "### 12.3. [Make Predictions](#make-predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-veteran",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions_002 = model_exp002.predict(dtest)\n",
    "predictions_002_train = model_exp002.predict(dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-margin",
   "metadata": {},
   "source": [
    "<a id=\"model-evaluation-on-test-split\"></a>\n",
    "\n",
    "## 13. [Model Evaluation on Test Split](#model-evaluation-on-test-split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-commodity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_pred = get_xgboost_preds_obs(y_test, predictions_002).set_index(\"Date_x\")\n",
    "df_pred[\"Sales_Raw\"] = np.expm1(df_pred[\"Sales\"])\n",
    "df_pred[\"pred_Raw\"] = np.expm1(df_pred[\"pred\"])\n",
    "df_pred[\"res\"] = df_pred[\"pred_Raw\"] - df_pred[\"Sales_Raw\"]\n",
    "\n",
    "df_pred_train = get_xgboost_preds_obs(y_train, predictions_002_train).set_index(\n",
    "    \"Date_x\"\n",
    ")\n",
    "df_pred_train[\"Sales_Raw\"] = np.expm1(df_pred_train[\"Sales\"])\n",
    "df_pred_train[\"pred_Raw\"] = np.expm1(df_pred_train[\"pred\"])\n",
    "df_pred_train[\"res\"] = df_pred_train[\"pred_Raw\"] - df_pred_train[\"Sales_Raw\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-easter",
   "metadata": {},
   "source": [
    "<a id=\"evaluation-metric\"></a>\n",
    "\n",
    "### 13.1. [Evaluation Metric](#evaluation-metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-stuart",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metric_test = rmspe(df_pred[\"pred\"], df_pred[\"Sales\"])\n",
    "metric_train = rmspe(df_pred_train[\"pred\"], df_pred_train[\"Sales\"])\n",
    "rmse_test = rmse(df_pred[\"pred\"], df_pred[\"Sales\"])\n",
    "rmse_train = rmse(df_pred_train[\"pred\"], df_pred_train[\"Sales\"])\n",
    "d_metrics = {\n",
    "    \"rmse_train\": rmse_train,\n",
    "    \"rmse_test\": rmse_test,\n",
    "    \"rmspe_train\": metric_train,\n",
    "    \"rmspe_test\": metric_test,\n",
    "}\n",
    "df_metrics = pd.DataFrame.from_dict(d_metrics, orient=\"index\").T\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-conservative",
   "metadata": {},
   "source": [
    "### Observed and Predicted, as a function of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-alexander",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "np.expm1(df_pred[[\"Sales\", \"pred\"]]).head(25_000).plot(ax=ax)\n",
    "ax.grid()\n",
    "ax.set_xlabel(None)\n",
    "ax.legend(ncol=2, bbox_to_anchor=(0.8, 1.1125), loc=\"upper left\", frameon=False)\n",
    "ax.set_title(\"Obs vs Pred\", loc=\"left\", fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-watch",
   "metadata": {},
   "source": [
    "<a id=\"distribution-of-residuals\"></a>\n",
    "\n",
    "### 13.2. [Distribution of Residuals](#distribution-of-residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-wrist",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "df_pred[\"res\"].plot.hist(edgecolor=\"white\", color=\"blue\", ax=ax)\n",
    "ax.set_ylabel(None)\n",
    "ax.grid(color=\"lightgrey\", alpha=1)\n",
    "ax.set_title(\"Distribution of Residuals\", fontweight=\"bold\", loc=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-penalty",
   "metadata": {},
   "source": [
    "<a id=\"prediction-error---observed-vs-predicted\"></a>\n",
    "\n",
    "### 13.3. [Prediction Error - Observed vs Predicted](#prediction-error---observed-vs-predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-duration",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(df_pred[[\"pred_Raw\"]], df_pred[\"Sales_Raw\"])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "df_pred[[\"Sales_Raw\", \"pred_Raw\"]].plot.scatter(\n",
    "    x=\"pred_Raw\", y=\"Sales_Raw\", color=\"white\", edgecolors=\"b\", s=40, ax=ax\n",
    ")\n",
    "ax.plot(\n",
    "    df_pred[\"pred_Raw\"].to_numpy(),\n",
    "    reg.predict(df_pred[[\"pred_Raw\"]]),\n",
    "    label=f\"best fit (R2={reg.score(df_pred[['pred_Raw']], df_pred['Sales_Raw']):.2f})\",\n",
    "    color=\"darkred\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(None)\n",
    "ax.grid(color=\"lightgrey\", alpha=0.75)\n",
    "ax.set_title(\n",
    "    f\"Observed vs Predicted (RMSPE={metric_test:.2f})\", fontweight=\"bold\", loc=\"left\"\n",
    ")\n",
    "ax.axline([0, 0], [1, 1], label=\"identity\", color=\"darkorange\", linestyle=\"--\")\n",
    "ax.legend(frameon=False)\n",
    "ax.set_xlim((0, None))\n",
    "ax.set_ylim((0, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-honor",
   "metadata": {},
   "source": [
    "<a id=\"residuals-vs-observed\"></a>\n",
    "\n",
    "### 13.4. [Residuals vs Observed](#residuals-vs-observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-whale",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "df_pred.loc[df_pred[\"res\"] > -10_000][[\"Sales_Raw\", \"res\"]].plot.scatter(\n",
    "    x=\"Sales_Raw\", y=\"res\", color=\"white\", edgecolors=\"b\", s=40, ax=ax\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(None)\n",
    "ax.axline([0, 0], [1, 0], color=\"black\", linestyle=\"--\")\n",
    "ax.grid(color=\"lightgrey\", alpha=0.75)\n",
    "ax.set_title(\"Residual vs Observed\", fontweight=\"bold\", loc=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-witness",
   "metadata": {},
   "source": [
    "<a id=\"quantile-quantile-plot-for-residuals\"></a>\n",
    "\n",
    "### 13.5. [Quantile-Quantile Plot for Residuals](#quantile-quantile-plot-for-residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-bloom",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_qq(\n",
    "    df_pred[\"res\"],\n",
    "    fig_size=(6, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-platinum",
   "metadata": {},
   "source": [
    "<a id=\"export-trained-ml-model\"></a>\n",
    "\n",
    "## 14. [Export trained ML model](#export-trained-ml-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-warner",
   "metadata": {},
   "source": [
    "<a id=\"train-ml-model-with-all-available-training-data-and-no-early-stopping\"></a>\n",
    "\n",
    "### 14.1. [Train ML model with all available training data and no early-stopping](#train-ml-model-with-all-available-training-data-and-no-early-stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-overall",
   "metadata": {},
   "source": [
    "Next, get the actual number of rounds where early stopping was triggered and use this for training the model on all the available training data - at this time, early stopping rounds will no longer be required since we know the exact round (`num_boost_round_best` below) when the reduction in loss has stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-dragon",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_boost_round_best = model_exp002.best_iteration + 1\n",
    "print(num_boost_round_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-costs",
   "metadata": {},
   "source": [
    "Next, train the model on all the available training data. This means re-running all the processing steps in this notebook (listed below, excluding those that have a line drawn through them since no testing data willl be available and no early stopping is required), using the full training dataset\n",
    "- Extract Features and Target from full training data\n",
    "- Handle missing values in full training data - Use Median of full training data\n",
    "- Encode Categorical Features\n",
    "- Scaling Numerical Features\n",
    "- Transform Target\n",
    "- Train with Best HyperParameters and Export to disk in preparation for inference\n",
    "  - Train with best hyper-parameters\n",
    "  - ~~XGBoost Training Curves~~\n",
    "  - ~~Make predictions~~\n",
    "  - Export trained ML model to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-appendix",
   "metadata": {},
   "source": [
    "<span style='color:red'><b>To be done</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-mother",
   "metadata": {},
   "source": [
    "<a id=\"export-ml-model-to-file\"></a>\n",
    "\n",
    "### 14.2. [Export ML model to file](#export-ml-model-to-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-superior",
   "metadata": {},
   "source": [
    "Finally, [export the model to a `pickle` file](https://cloud.google.com/ai-platform/prediction/docs/getting-started-scikit-xgboost#xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model_exp002, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-luxury",
   "metadata": {},
   "source": [
    "<a id=\"future-work\"></a>\n",
    "\n",
    "## 15. [Future Work](#future-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-engine",
   "metadata": {},
   "source": [
    "1.  <span style='color:red'><b>(IMPORTANT) Scale features (standardization or normalization)</b></span> during the following steps\n",
    "    -   [final model training](#scaling-numerical-features)\n",
    "    -   [cross-validation](#hyper-parameter-tuning-with-k-fold-cross-validation)\n",
    "2.  Try making the regression target (`y`) stationary (in a timeseries sense)\n",
    "3.  Examine prediction errors (eg. over- or under-predictions), grouped by\n",
    "    -   store\n",
    "    -   state\n",
    "    -   school holiday\n",
    "    -   state holiday\n",
    "    -   other categorical features\n",
    "4.  Explore XGBoost Hyper-Parameter settings during [cross-validation](#hyper-parameter-tuning-with-k-fold-cross-validation)\n",
    "    -   increase number of estimators (`num_boosting_rounds`)\n",
    "    -   modify `early_stopping_rounds`\n",
    "    -   other XGBoost hyper-parameters (`eta`, `max_depth`)\n",
    "5.  Although use-case calls for single-shot forecast, also consider a rolling forecast using ML algorithms (such as XGBoost, etc.) using one of the following two approaches\n",
    "    -   recursive\n",
    "        -   predict one-step ahead and add predicted value to training data for predicting next step\n",
    "        -   disadvantage\n",
    "            -   quickly accumulates prediction errors at each step since previously predicted values are used to build features for current step prediction\n",
    "            -   not the best for long forecast horizons where accumulated error may render forecast unusable\n",
    "    -   direct\n",
    "        -   shift timeseries ahead by required horizon into future and then train model and make multi-step predictions for each step comprising the forecast horizon\n",
    "        -   disadvantage\n",
    "            -   can't model relationship between predictions since model for each step is independent\n",
    "            -   larger computational cost so it is not suitable for long forecast horizons\n",
    "            -   introduces a gap to be left **after** training data, of the same length as the forecast horizon, before first forecasted value is available\n",
    "                -   this means that the training data set would need to end on June 14, 2015, which is six weeks (length of forecast horizon) before current end date (July 25, 2015) of training data and the first forecasted date will be July 25, 2015\n",
    "\n",
    "    In both of these rolling forecast approaches, the forecast will need to also cover the gap (July 26, 2015 - July 31, 2015) period between the end of the training and start of the unknown (real) data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-choir",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-track",
   "metadata": {},
   "source": [
    "<span style=\"float:left;\">\n",
    "    <a href=\"./1_data_clean_feat_eng.ipynb\"><< 1 - Data Cleaning and Feature Engineering</a>\n",
    "</span>\n",
    "\n",
    "<span style=\"float:right;\">\n",
    "    &#169; 2021 | <a href=\"https://github.com/edesz/streetcar-delays\">@edesz</a> (MIT)\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
