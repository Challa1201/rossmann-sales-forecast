{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aboriginal-calculator",
   "metadata": {},
   "source": [
    "# XGBoost Regression Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-wyoming",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-looking",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from glob import glob\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, plotting\n",
    "from hyperopt.pyll.base import scope\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-tactics",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%aimport src.ml_custom_transformers\n",
    "import src.ml_custom_transformers as ct\n",
    "\n",
    "%aimport src.xgboost_helpers\n",
    "from src.xgboost_helpers import get_xgboost_training_curves\n",
    "\n",
    "%aimport src.metrics_helpers\n",
    "from src.metrics_helpers import rmspe_xg, rmspe, rmse\n",
    "\n",
    "%aimport src.utils\n",
    "from src.utils import get_xgboost_preds_obs\n",
    "\n",
    "%aimport src.data_helpers\n",
    "from src.data_helpers import (\n",
    "    MultipleTimeSeriesValCV,\n",
    "    sample_train_val_split,\n",
    "    split_data,\n",
    ")\n",
    "\n",
    "%aimport src.test_helpers\n",
    "from src.test_helpers import (\n",
    "    test_dfgetxy,\n",
    "    test_split_data,\n",
    "    test_fillna,\n",
    "    test_sampled_data_sizes,\n",
    "    test_target_encode_categorical_features,\n",
    "    test_log1p,\n",
    ")\n",
    "\n",
    "%aimport src.hyperopt_helpers\n",
    "from src.hyperopt_helpers import summarize_hyperopt_trials\n",
    "\n",
    "%aimport src.visualization_helpers\n",
    "from src.visualization_helpers import plot_qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-accordance",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0.  [About](#about)\n",
    "1.  [User Inputs](#user-inputs)\n",
    "2.  [Load Data](#load-data)\n",
    "3.  [Preprocessing - Set index for Data](#preprocessing---set-index-for-data)\n",
    "4.  [Create Training, Validation and Testing Splits from Training Data](#create-training-validation-and-testing-splits-from-training-data)\n",
    "5.  [Show `datatype`s and Missing Values](#show-`datatype`s-and-missing-values)\n",
    "    -   5.1. [Inspect Columns with Missing Values](#inspect-columns-with-missing-values)\n",
    "6.  [Hyper-Parameter Tuning with K-Fold Cross-Validation](#hyper-parameter-tuning-with-k-fold-cross-validation)\n",
    "    -   6.1. [Check Data Processing during Cross-Validation](#check-data-processing-during-cross-validation)\n",
    "    -   6.2. [Tune HyperParameters](#tune-hyperparameters)\n",
    "    -   6.3. [Get Best HyperParameters](#get-best-hyperparameters)\n",
    "7. [Extract Features and Target from all Splits](#extract-features-and-target-from-all-splits)\n",
    "8. [Handle missing values in each split - Use Median of Training Data](#handle-missing-values-in-each-split---use-median-of-training-data)\n",
    "9.  [Encode Categorical Features](#encode-categorical-features)\n",
    "10.  [Scaling Numerical Features](#scaling-numerical-features)\n",
    "11.  [Transform Target](#transform-target)\n",
    "12.  [Train-Predict with Best HyperParameters](#train-predict-with-best-hyperparameters)\n",
    "     -   12.1. [Train with best hyper-parameters](#train-with-best-hyper-parameters)\n",
    "     -   12.2. [XGBoost Training Curves - Training and Validation Splits](#xgboost-training-curves---training-and-validation-splits)\n",
    "     -   12.3. [Make Predictions](#make-predictions)\n",
    "13.  [Model Evaluation on Test Split](#model-evaluation-on-test-split)\n",
    "     -   13.1. [Evaluation Metric](#evaluation-metric)\n",
    "     -   13.2. [Distribution of Residuals](#distribution-of-residuals)\n",
    "     -   13.3. [Prediction Error - Observed vs Predicted](#prediction-error---observed-vs-predicted)\n",
    "     -   13.4. [Residuals vs Observed](#residuals-vs-observed)\n",
    "     -   13.5. [Quantile-Quantile Plot for Residuals](#quantile-quantile-plot-for-residuals)\n",
    "14.  [Export trained ML model](#export-trained-ml-model)\n",
    "     -   16.1. [Train ML model with all available training data and no early-stopping](#train-ml-model-with-all-available-training-data-and-no-early-stopping)\n",
    "     -   16.2. [Export ML model to file](#export-ml-model-to-file)\n",
    "15. [Notes](#notes)\n",
    "16. [Future Work](#future-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-transcription",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-17T14:26:32.381936Z",
     "iopub.status.busy": "2021-03-17T14:26:32.381752Z",
     "iopub.status.idle": "2021-03-17T14:26:32.387662Z",
     "shell.execute_reply": "2021-03-17T14:26:32.387012Z",
     "shell.execute_reply.started": "2021-03-17T14:26:32.381889Z"
    }
   },
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-treat",
   "metadata": {},
   "source": [
    "In this notebook, we'll run ML regression trials using XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-desire",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-freedom",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJ_ROOT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-iceland",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"Store\",\n",
    "    \"DayOfWeek\",\n",
    "    \"Year\",\n",
    "    \"Month\",\n",
    "    \"Day\",\n",
    "    \"StateHoliday\",\n",
    "    \"CompetitionMonthsOpen\",\n",
    "    \"Promo2Weeks\",\n",
    "    \"StoreType\",\n",
    "    \"Assortment\",\n",
    "    \"PromoInterval\",\n",
    "    \"CompetitionOpenSinceYear\",\n",
    "    \"Promo2SinceYear\",\n",
    "    \"State\",\n",
    "    \"Week\",\n",
    "    \"Events\",\n",
    "    \"Promo_fw\",\n",
    "    \"Promo_bw\",\n",
    "    \"StateHoliday_fw\",\n",
    "    \"StateHoliday_bw\",\n",
    "    \"SchoolHoliday_fw\",\n",
    "    \"SchoolHoliday_bw\",\n",
    "    \"CompetitionDistance\",\n",
    "    \"Max_TemperatureC\",\n",
    "    \"Mean_TemperatureC\",\n",
    "    \"Min_TemperatureC\",\n",
    "    \"Max_Humidity\",\n",
    "    \"Mean_Humidity\",\n",
    "    \"Min_Humidity\",\n",
    "    \"Max_Wind_SpeedKm_h\",\n",
    "    \"Mean_Wind_SpeedKm_h\",\n",
    "    \"CloudCover\",\n",
    "    # \"trend\",\n",
    "    # \"trend_DE\",\n",
    "    \"AfterStateHoliday\",\n",
    "    \"BeforeStateHoliday\",\n",
    "    \"Promo\",\n",
    "    \"SchoolHoliday\",\n",
    "    \"Date\",\n",
    "    \"Sales\",\n",
    "]\n",
    "continuous_vars_missing_vals = [\"CompetitionDistance\", \"CloudCover\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-distributor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_data_path = os.path.join(PROJ_ROOT_DIR, \"data\", \"processed\")\n",
    "train_parquet_filepath = os.path.join(\n",
    "    processed_data_path, \"cleaned_train\" + \"_*\" + \".parquet.gzip\"\n",
    ")\n",
    "holdout_parquet_filepath = os.path.join(\n",
    "    processed_data_path, \"cleaned_test\" + \"_*\" + \".parquet.gzip\"\n",
    ")\n",
    "train_parquet_full_filepath = glob(train_parquet_filepath)[0]\n",
    "print(train_parquet_full_filepath)\n",
    "holdout_parquet_full_filepath = glob(holdout_parquet_filepath)[0]\n",
    "print(holdout_parquet_full_filepath)\n",
    "\n",
    "seed = 123\n",
    "space = {\n",
    "    # Learning rate: default 0.3 -> range: [0,3]\n",
    "    \"eta\": hp.quniform(\"eta\", 0.01, 0.3, 0.001),\n",
    "    # Control complexity (control overfitting)\n",
    "    # Maximum depth of a tree: default 6 -> range: [0:∞]\n",
    "    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 5, 10, 1)),\n",
    "    # Minimum sum of instance weight (hessian) needed in a child: default 1\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 3, 1),\n",
    "    # Minimum loss reduction required: default 0 -> range: [0,∞]\n",
    "    \"gamma\": hp.quniform(\"gamma\", 0, 5, 0.5),\n",
    "    # Add randomness to make training robust to noise (control overfitting)\n",
    "    # Subsample ratio of the training instance: default 1\n",
    "    \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.05),\n",
    "    # Subsample ratio of columns when constructing each tree: default 1\n",
    "    \"colsample_bytree\": hp.quniform(\"colsample_bytree\", 0.5, 1, 0.05),\n",
    "    # Regression problem\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    # For reproducibility\n",
    "    \"seed\": seed,\n",
    "    # Faster computation = gpu_hist\n",
    "    \"tree_method\": \"hist\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-challenge",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-fountain",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_score():\n",
    "    _ = score(space, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-expense",
   "metadata": {},
   "source": [
    "<a id=\"load-data\"></a>\n",
    "\n",
    "## 2. [Load Data](#load-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-sunday",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 1) Load data\n",
    "tunning_dataset = pd.read_parquet(train_parquet_full_filepath, columns=columns)\n",
    "holdout_dataset = pd.read_parquet(\n",
    "    holdout_parquet_full_filepath, columns=list(set(columns) - (set([\"Sales\"])))\n",
    ")\n",
    "print(tunning_dataset.shape)\n",
    "print(holdout_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-grenada",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing---set-index-for-data\"></a>\n",
    "\n",
    "## 3. [Preprocessing - Set index for Data](#preprocessing---set-index-for-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-culture",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 2) Let's use the date as the index and sort the data\n",
    "# tunning_dataset.sort_values(\"Date\", inplace=True)\n",
    "# tunning_dataset.set_index(\"Date\", inplace=True)\n",
    "# columns.remove(\"Date\")\n",
    "\n",
    "# tunning_dataset_X = tunning_dataset\n",
    "# tunning_dataset_y = tunning_dataset_X.pop(\"Sales\")\n",
    "\n",
    "# # 2) Let's use the date as the index and sort the data\n",
    "# holdout_dataset.sort_values(\"Date\", inplace=True)\n",
    "# holdout_dataset.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# holdout_dataset_X = holdout_dataset\n",
    "# print(tunning_dataset.shape, tunning_dataset_X.shape, tunning_dataset_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-friend",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "setup_pipe = Pipeline(\n",
    "    [\n",
    "        (\"datesort\", ct.DFSortByDateSetDateIndex(\"Date\")),\n",
    "    ]\n",
    ")\n",
    "tunning_dataset = setup_pipe.fit_transform(tunning_dataset)\n",
    "print(tunning_dataset.shape)\n",
    "holdout_dataset = setup_pipe.fit_transform(holdout_dataset)\n",
    "print(holdout_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-gregory",
   "metadata": {},
   "source": [
    "<a id=\"create-training-validation-and-testing-splits-from-training-data\"></a>\n",
    "\n",
    "## 4. [Create Training, Validation and Testing Splits from Training Data](#create-training-validation-and-testing-splits-from-training-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-dylan",
   "metadata": {},
   "source": [
    "Although a holdout set is provided, the target value is not known there. So, we'll divide the overall training data into training, validation and testing splits, from the training data, in order to assist in model assessment.\n",
    "\n",
    "Hyper-parameter tuning will be preformed using the training split, while the validation and testing split will only be used for assessment (with the best hyper-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-scale",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "d = split_data(tunning_dataset, holdout_dataset)\n",
    "train_index, val_index, test_index = d[\"indexes\"]\n",
    "df_train, df_val, df_test = d[\"splits\"]\n",
    "test_split_data(\n",
    "    train_index,\n",
    "    val_index,\n",
    "    test_index,\n",
    "    len(holdout_dataset.index.unique()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-gentleman",
   "metadata": {},
   "source": [
    "<a id=\"show-`datatype`s-and-missing-values\"></a>\n",
    "\n",
    "## 5. [Show `datatype`s and Missing Values](#show-`datatype`s-and-missing-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-plant",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_summary = (\n",
    "    df_train.isna()\n",
    "    .sum()\n",
    "    .to_frame()\n",
    "    .rename(columns={0: \"NaNs\"})\n",
    "    .merge(\n",
    "        df_train.dtypes.to_frame().rename(columns={0: \"dtype\"}),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    ")\n",
    "display(df_train.head())\n",
    "display(df_train_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-suspension",
   "metadata": {},
   "source": [
    "<a id=\"inspect-columns-with-missing-values\"></a>\n",
    "\n",
    "### 5.1. [Inspect Columns with Missing Values](#inspect-columns-with-missing-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-kruger",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train_with_nans = df_train[df_train.columns[(df_train.isna().sum() > 0).tolist()]]\n",
    "display(df_train_with_nans.dtypes.to_frame().T)\n",
    "display(df_train_with_nans.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-cleaner",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 3) Apply log transform\n",
    "# tunning_dataset_y = np.log1p(tunning_dataset_y)\n",
    "\n",
    "# # Number of cross-validation folds (from the last)\n",
    "# pred_folds = 3\n",
    "# train_times = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-tanzania",
   "metadata": {},
   "source": [
    "<a id=\"hyper-parameter-tuning-with-k-fold-cross-validation\"></a>\n",
    "\n",
    "## 6. [Hyper-Parameter Tuning with K-Fold Cross-Validation](#hyper-parameter-tuning-with-k-fold-cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-jacksonville",
   "metadata": {},
   "source": [
    "Examine the 10 splits, each covering 48 unique days worth of sales data for each store, that will be produced by the custom `datetime`-index based cross-validation splitter\n",
    "- a limitation of this splitter is that the length of the validation and test splits do not agree with eachother (they disagree by a few hundred samples per fold generated)\n",
    "  - during development of the splitter, it was intended that they be equal in length, but this has not proven to be the case\n",
    "  - future work should address this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-poultry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cv = MultipleTimeSeriesValCV(10, 48, 1)\n",
    "for cv_fold_idx, (train_index, val_index, test_index) in enumerate(cv.split(df_train)):\n",
    "    train = df_train.iloc[train_index]\n",
    "    train_dates = train.index.get_level_values(\"Date\").unique()\n",
    "    val = df_train.iloc[val_index]\n",
    "    val_dates = val.index.get_level_values(\"Date\").unique()\n",
    "    test = df_train.iloc[test_index]\n",
    "    test_dates = test.index.get_level_values(\"Date\").unique()\n",
    "    print(\n",
    "        cv_fold_idx+1,\n",
    "        len(train_dates),\n",
    "        len(val_dates),\n",
    "        len(test_dates),\n",
    "        len(train),\n",
    "        len(val),\n",
    "        len(test),\n",
    "        train_dates.min().strftime(\"%Y-%m-%d\"),\n",
    "        train_dates.max().strftime(\"%Y-%m-%d\"),\n",
    "        val_dates.min().strftime(\"%Y-%m-%d\"),\n",
    "        val_dates.max().strftime(\"%Y-%m-%d\"),\n",
    "        test_dates.min().strftime(\"%Y-%m-%d\"),\n",
    "        test_dates.max().strftime(\"%Y-%m-%d\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-invasion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score(params, check_data_run=False):\n",
    "    n_split = 10\n",
    "    train_sample_size = 5_000\n",
    "    val_sample_size = 2_500\n",
    "    rmspe_test_fold_scores = []\n",
    "    cv = MultipleTimeSeriesValCV(n_split, 48, 1)\n",
    "    for cv_fold_idx, (train_index, val_index, test_index) in enumerate(\n",
    "        cv.split(df_train)\n",
    "    ):\n",
    "        assert cv_fold_idx + 1 <= n_split\n",
    "        assert train_sample_size <= len(val_index)\n",
    "\n",
    "        # 5) Select a random sample for early stopping\n",
    "        # - to do this, use the first 2,500 samples of the training\n",
    "        # split for training and the next 2,500 for validation\n",
    "        # - when using samples, the validation split (produced by\n",
    "        # the cv splitter) will not be used\n",
    "        (\n",
    "            train_train_index,\n",
    "            train_es_index,\n",
    "            test_index_sampled,\n",
    "            train_full_index,\n",
    "        ) = sample_train_val_split(\n",
    "            len(train_index), len(test_index), train_sample_size, val_sample_size\n",
    "        )\n",
    "        df_train_cv = df_train.iloc[train_train_index]\n",
    "        df_val_cv = df_train.iloc[train_es_index]\n",
    "        df_test_cv = df_train.iloc[test_index_sampled]\n",
    "        if check_data_run:\n",
    "            print(\n",
    "                f\"k={cv_fold_idx}, \"\n",
    "                f\"Train={len(train_index)}, \"\n",
    "                f\"Train_Sampled={len(df_train_cv)}, \"\n",
    "                f\"Val={len(train_index)}, \"\n",
    "                f\"Val_Sampled={len(df_train_cv)}, \"\n",
    "                f\"Test={len(test_index)}, \"\n",
    "                f\"TestSampled={len(df_test_cv)}\"\n",
    "            )\n",
    "        # print(min(train_train_index), max(train_train_index))\n",
    "        # print(min(train_es_index), max(train_es_index))\n",
    "        # print(min(test_index_sampled), max(test_index_sampled))\n",
    "        test_sampled_data_sizes(\n",
    "            train_train_index,\n",
    "            train_es_index,\n",
    "            test_index_sampled,\n",
    "            val_sample_size,\n",
    "            train_full_index,\n",
    "            df_train_cv,\n",
    "            df_val_cv,\n",
    "            df_test_cv,\n",
    "        )\n",
    "\n",
    "        # 4) Extract features and target (by index) from CV splits\n",
    "        # X_train, X_val, X_test, y_train, y_val, y_test = (\n",
    "        #     tunning_dataset_X.iloc[train_train_index].copy(),\n",
    "        #     tunning_dataset_X.iloc[train_es_index].copy(),\n",
    "        #     tunning_dataset_X.iloc[test_index_sampled].copy(),\n",
    "        #     tunning_dataset_y.iloc[train_train_index].copy(),\n",
    "        #     tunning_dataset_y.iloc[train_es_index].copy(),\n",
    "        #     tunning_dataset_y.iloc[test_index_sampled].copy(),\n",
    "        # )\n",
    "        getxy_pipe = Pipeline(\n",
    "            [\n",
    "                (\"getxy\", ct.DFGetXY(\"Sales\")),\n",
    "            ]\n",
    "        )\n",
    "        X_train, y_train = getxy_pipe.fit_transform(df_train_cv.copy())\n",
    "        X_val, y_val = getxy_pipe.fit_transform(df_val_cv.copy())\n",
    "        X_test, y_test = getxy_pipe.fit_transform(df_test_cv.copy())\n",
    "        test_dfgetxy(X_train, y_train, \"Sales\")\n",
    "        test_dfgetxy(X_val, y_val, \"Sales\")\n",
    "        test_dfgetxy(X_test, y_test, \"Sales\")\n",
    "\n",
    "        # 5) Replace missing data in numerical features\n",
    "        # for col_name in continuous_vars_missing_vals:\n",
    "        #     # Add na cols\n",
    "        #     X_train[col_name + \"_na\"] = pd.isnull(X_train[col_name])\n",
    "        #     X_val[col_name + \"_na\"] = pd.isnull(X_val[col_name])\n",
    "        #     X_test[col_name + \"_na\"] = pd.isnull(X_test[col_name])\n",
    "        #     # Fill missing with median\n",
    "        #     fillter = X_train[col_name].median()\n",
    "        #     X_train[col_name] = X_train[col_name].fillna(fillter)\n",
    "        #     X_val[col_name] = X_val[col_name].fillna(fillter)\n",
    "        #     X_test[col_name] = X_test[col_name].fillna(fillter)\n",
    "        fillna_pipe = Pipeline(\n",
    "            [\n",
    "                (\"cd\", ct.DFFillNa(\"CompetitionDistance\")),\n",
    "                (\"cda\", ct.DFAddNaIndicator(\"CompetitionDistance\")),\n",
    "                (\"cc\", ct.DFFillNa(\"CloudCover\")),\n",
    "                (\"cca\", ct.DFAddNaIndicator(\"CloudCover\")),\n",
    "            ]\n",
    "        )\n",
    "        fillna_pipe.fit(X_train)\n",
    "        X_train = fillna_pipe.transform(X_train)\n",
    "        X_val = fillna_pipe.transform(X_val)\n",
    "        X_test = fillna_pipe.transform(X_test)\n",
    "        # assert (X_train[continuous_vars_missing_vals].isna().sum().tolist()) == [\n",
    "        #     0\n",
    "        # ] * len(continuous_vars_missing_vals)\n",
    "        test_fillna(X_train, X_val, X_test, continuous_vars_missing_vals)\n",
    "\n",
    "        # 6) Handle categorical features (missing data)\n",
    "        # cat_vars = list(X_train.select_dtypes(include=\"object\"))\n",
    "        # te = TargetEncoder(handle_missing=\"value\")\n",
    "        # X_train = te.fit_transform(X_train, cols=cat_vars, y=y_train)\n",
    "        # X_val = te.transform(X_val)\n",
    "        # X_test = te.transform(X_test)\n",
    "        cat_vars = list(X_train.select_dtypes(include=\"object\"))\n",
    "        cat_pipe = Pipeline(\n",
    "            [\n",
    "                (\"targetcats\", ct.DFTargetEncodeCategoricalFeatures(cat_vars)),\n",
    "            ]\n",
    "        )\n",
    "        cat_pipe.fit(X_train, y_train)\n",
    "        X_train = cat_pipe.transform(X_train)\n",
    "        X_val = cat_pipe.transform(X_val)\n",
    "        X_test = cat_pipe.transform(X_test)\n",
    "        # assert not list(X_train.select_dtypes(include=\"object\"))\n",
    "        # assert not list(X_val.select_dtypes(include=\"object\"))\n",
    "        # assert not list(X_test.select_dtypes(include=\"object\"))\n",
    "        test_target_encode_categorical_features(X_train, X_val, X_test)\n",
    "\n",
    "        # 3) Apply log transform to the regression target in order to\n",
    "        # improve the shape of its skewed distribution\n",
    "        # y_train = np.log1p(y_train)\n",
    "        # y_val = np.log1p(y_val)\n",
    "        # y_test = np.log1p(y_test)\n",
    "        log1p_pipe = Pipeline(\n",
    "            [\n",
    "                (\"log1p\", ct.DFLog1p(\"Sales\")),\n",
    "            ]\n",
    "        )\n",
    "        y_train = log1p_pipe.fit_transform(y_train.to_frame()).squeeze()\n",
    "        y_val = log1p_pipe.fit_transform(y_val.to_frame()).squeeze()\n",
    "        y_test = log1p_pipe.fit_transform(y_test.to_frame()).squeeze()\n",
    "        test_log1p([y_train, y_val, y_test])\n",
    "\n",
    "        # 7) Convert data splits to a DMatrix for XGBoost Learning API\n",
    "        dtrain = xgb.DMatrix(X_train, y_train)\n",
    "        dvalid = xgb.DMatrix(X_val, y_val)\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "        watchlist = [(dtrain, \"train\"), (dvalid, \"eval\")]\n",
    "\n",
    "        if not check_data_run:\n",
    "            # Can use feval for a custom objective function\n",
    "            model = xgb.train(\n",
    "                params,\n",
    "                dtrain,\n",
    "                early_stopping_rounds=50,\n",
    "                num_boost_round=400,\n",
    "                verbose_eval=False,\n",
    "                feval=rmspe_xg,\n",
    "                evals=watchlist,\n",
    "            )\n",
    "            # evaluate model on the test data produced by the CV splitter\n",
    "            y_pred = model.predict(dtest)\n",
    "            rmspe_test = rmspe(y_test, y_pred)\n",
    "\n",
    "            rmspe_test_fold_scores.append(rmspe_test)\n",
    "\n",
    "    if not check_data_run:\n",
    "        return np.mean(rmspe_test_fold_scores)\n",
    "    return [X_train, X_val, X_test, y_train, y_val, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-touch",
   "metadata": {},
   "source": [
    "<a id=\"check-data-processing-during-cross-validation\"></a>\n",
    "\n",
    "### 6.1. [Check Data Processing during Cross-Validation](#check-data-processing-during-cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-relaxation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "check_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-chicago",
   "metadata": {},
   "source": [
    "<a id=\"tune-hyperparameters\"></a>\n",
    "\n",
    "### 6.2. [Tune HyperParameters](#tune-hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-tokyo",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# trials will contain logging information\n",
    "trials = Trials()\n",
    "\n",
    "# tune hyper-parameters\n",
    "best_hyperparams = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=100)\n",
    "print(f\"Best hyper-parameters: {best_hyperparams}\")\n",
    "display(summarize_hyperopt_trials(trials))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-lebanon",
   "metadata": {},
   "source": [
    "<a id=\"get-best-hyperparameters\"></a>\n",
    "\n",
    "### 6.3. [Get Best HyperParameters](#get-best-hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-portuguese",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_hyperparams = {}\n",
    "best_hyperparams[\"objective\"] = \"reg:squarederror\"\n",
    "best_hyperparams[\"seed\"] = seed\n",
    "best_hyperparams[\"tree_method\"] = \"hist\"\n",
    "best_hyperparams[\"max_depth\"] = int(best_hyperparams[\"max_depth\"])\n",
    "# best_hyperparams[\"max_depth\"] = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-grass",
   "metadata": {},
   "source": [
    "<a id=\"extract-features-and-target-from-all-splits\"></a>\n",
    "\n",
    "## 7. [Extract Features and Target from all Splits](#extract-features-and-target-from-all-splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-chance",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# X_train, X_val, X_test = (\n",
    "#     tunning_dataset_X.iloc[train_index].copy(),\n",
    "#     tunning_dataset_X.iloc[val_index].copy(),\n",
    "#     tunning_dataset_X.iloc[test_index].copy(),\n",
    "# )\n",
    "# y_train, y_val, y_test = (\n",
    "#     tunning_dataset_y.iloc[train_index].copy(),\n",
    "#     tunning_dataset_y.iloc[val_index].copy(),\n",
    "#     tunning_dataset_y.iloc[test_index].copy(),\n",
    "# )\n",
    "# print(\n",
    "#     X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-server",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "getxy_pipe = Pipeline(\n",
    "    [\n",
    "        (\"getxy\", ct.DFGetXY(\"Sales\")),\n",
    "    ]\n",
    ")\n",
    "X_train, y_train = getxy_pipe.fit_transform(df_train.copy())\n",
    "X_val, y_val = getxy_pipe.fit_transform(df_val.copy())\n",
    "X_test, y_test = getxy_pipe.fit_transform(df_test.copy())\n",
    "print(\n",
    "    X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-theater",
   "metadata": {},
   "source": [
    "<a id=\"handle-missing-values-in-each-split---use-median-of-training-data\"></a>\n",
    "\n",
    "## 8. [Handle missing values in each split - Use Median of Training Data](#handle-missing-values-in-each-split---use-median-of-training-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-things",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for col_name in [\"CompetitionDistance\", \"CloudCover\"]:\n",
    "#     # Add na cols\n",
    "#     X_train[col_name + \"_na\"] = pd.isnull(X_train[col_name])\n",
    "#     X_val[col_name + \"_na\"] = pd.isnull(X_val[col_name])\n",
    "#     X_test[col_name + \"_na\"] = pd.isnull(X_test[col_name])\n",
    "#     # Fill missing with median (default in FastAI) of TRAINING set\n",
    "#     fillter = X_train[col_name].median()\n",
    "#     X_train[col_name] = X_train[col_name].fillna(fillter)\n",
    "#     X_val[col_name] = X_val[col_name].fillna(fillter)\n",
    "#     X_test[col_name] = X_test[col_name].fillna(fillter)\n",
    "# assert (X_train[continuous_vars_missing_vals].isna().sum().tolist()) == [0] * len(\n",
    "#     continuous_vars_missing_vals\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-equipment",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fillna_pipe = Pipeline(\n",
    "    [\n",
    "        (\"cd\", ct.DFFillNa(\"CompetitionDistance\")),\n",
    "        (\"cda\", ct.DFAddNaIndicator(\"CompetitionDistance\")),\n",
    "        (\"cc\", ct.DFFillNa(\"CloudCover\")),\n",
    "        (\"cca\", ct.DFAddNaIndicator(\"CloudCover\")),\n",
    "    ]\n",
    ")\n",
    "fillna_pipe.fit(X_train)\n",
    "X_train = fillna_pipe.transform(X_train)\n",
    "X_val = fillna_pipe.transform(X_val)\n",
    "X_test = fillna_pipe.transform(X_test)\n",
    "test_fillna(X_train, X_val, X_test, continuous_vars_missing_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-rough",
   "metadata": {},
   "source": [
    "<a id=\"encode-categorical-features\"></a>\n",
    "\n",
    "## 9. [Encode Categorical Features](#encode-categorical-features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-nylon",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# cat_vars = list(X_train.select_dtypes(include=\"object\"))\n",
    "# te = TargetEncoder(handle_missing=\"value\")\n",
    "# X_train = te.fit_transform(X_train, cols=cat_vars, y=y_train)\n",
    "# X_val = te.transform(X_val)\n",
    "# X_test = te.transform(X_test)\n",
    "# assert not list(X_train.select_dtypes(include=\"object\"))\n",
    "# assert not list(X_val.select_dtypes(include=\"object\"))\n",
    "# assert not list(X_test.select_dtypes(include=\"object\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-terrain",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cat_vars = list(X_train.select_dtypes(include=\"object\"))\n",
    "cat_pipe = Pipeline(\n",
    "    [\n",
    "        (\"targetcats\", ct.DFTargetEncodeCategoricalFeatures(cat_vars)),\n",
    "    ]\n",
    ")\n",
    "cat_pipe.fit(X_train, y_train)\n",
    "X_train = cat_pipe.transform(X_train)\n",
    "X_val = cat_pipe.transform(X_val)\n",
    "X_test = cat_pipe.transform(X_test)\n",
    "test_target_encode_categorical_features(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-folder",
   "metadata": {},
   "source": [
    "<a id=\"scaling-numerical-features\"></a>\n",
    "\n",
    "## 10. [Scaling Numerical Features](#scaling-numerical-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-thunder",
   "metadata": {},
   "source": [
    "<span style='color:red'><b>(IMPORTANT) To be done</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-friendship",
   "metadata": {},
   "source": [
    "<a id=\"transform-target\"></a>\n",
    "\n",
    "## 11. [Transform Target](#transform-target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-current",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_train = np.log1p(y_train)\n",
    "# y_val = np.log1p(y_val)\n",
    "# y_test = np.log1p(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-attribute",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "log1p_pipe = Pipeline(\n",
    "    [\n",
    "        (\"log1p\", ct.DFLog1p(\"Sales\")),\n",
    "    ]\n",
    ")\n",
    "y_train = log1p_pipe.fit_transform(y_train.to_frame()).squeeze()\n",
    "y_val = log1p_pipe.fit_transform(y_val.to_frame()).squeeze()\n",
    "y_test = log1p_pipe.fit_transform(y_test.to_frame()).squeeze()\n",
    "test_log1p([y_train, y_val, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-mirror",
   "metadata": {},
   "source": [
    "<a id=\"train-predict-with-best-hyperparameters\"></a>\n",
    "\n",
    "## 12. [Train-Predict with Best HyperParameters](#train-predict-with-best-hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-frank",
   "metadata": {},
   "source": [
    "Convert overall data splits to XGBoost `DMatrix` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-desert",
   "metadata": {},
   "source": [
    "<a id=\"train-with-best-hyper-parameters\"></a>\n",
    "\n",
    "### 12.1. [Train with best hyper-parameters](#train-with-best-hyper-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-opera",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "training_curves_002 = {}\n",
    "model_exp002 = xgb.train(\n",
    "    params=best_hyperparams, \n",
    "    dtrain=dtrain, \n",
    "    num_boost_round=400, \n",
    "    early_stopping_rounds=50, \n",
    "    feval=rmspe_xg, \n",
    "    evals=watchlist, \n",
    "    evals_result=training_curves_002\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-arena",
   "metadata": {},
   "source": [
    "<a id=\"xgboost-training-curves---training-and-validation-splits\"></a>\n",
    "\n",
    "### 12.2. [XGBoost Training Curves - Training and Validation Splits](#xgboost-training-curves---training-and-validation-splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-relay",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "df_training_curves = get_xgboost_training_curves(training_curves_002, \"rmspe\")\n",
    "df_training_curves.plot(ax=ax)\n",
    "ax.grid()\n",
    "ax.legend(ncol=2, bbox_to_anchor=(0.8, 1.1), loc=\"upper left\", frameon=False)\n",
    "ax.set_title(\"RMSPE Loss vs Iterations\", loc=\"left\", fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-jersey",
   "metadata": {},
   "source": [
    "<a id=\"make-predictions\"></a>\n",
    "\n",
    "### 12.3. [Make Predictions](#make-predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-produce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions_002 = model_exp002.predict(dtest)\n",
    "predictions_002_train = model_exp002.predict(dtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-roots",
   "metadata": {},
   "source": [
    "<a id=\"model-evaluation-on-test-split\"></a>\n",
    "\n",
    "## 13. [Model Evaluation on Test Split](#model-evaluation-on-test-split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-factor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_pred = get_xgboost_preds_obs(y_test, predictions_002).set_index(\"Date_x\")\n",
    "df_pred[\"Sales_Raw\"] = np.expm1(df_pred[\"Sales\"])\n",
    "df_pred[\"pred_Raw\"] = np.expm1(df_pred[\"pred\"])\n",
    "df_pred[\"res\"] = df_pred[\"pred_Raw\"] - df_pred[\"Sales_Raw\"]\n",
    "\n",
    "df_pred_train = get_xgboost_preds_obs(y_train, predictions_002_train).set_index(\n",
    "    \"Date_x\"\n",
    ")\n",
    "df_pred_train[\"Sales_Raw\"] = np.expm1(df_pred_train[\"Sales\"])\n",
    "df_pred_train[\"pred_Raw\"] = np.expm1(df_pred_train[\"pred\"])\n",
    "df_pred_train[\"res\"] = df_pred_train[\"pred_Raw\"] - df_pred_train[\"Sales_Raw\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-petite",
   "metadata": {},
   "source": [
    "<a id=\"evaluation-metric\"></a>\n",
    "\n",
    "### 13.1. [Evaluation Metric](#evaluation-metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-scanning",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metric_test = rmspe(df_pred[\"pred\"], df_pred[\"Sales\"])\n",
    "metric_train = rmspe(df_pred_train[\"pred\"], df_pred_train[\"Sales\"])\n",
    "rmse_test = rmse(df_pred[\"pred\"], df_pred[\"Sales\"])\n",
    "rmse_train = rmse(df_pred_train[\"pred\"], df_pred_train[\"Sales\"])\n",
    "d_metrics = {\n",
    "    \"rmse_train\": rmse_train,\n",
    "    \"rmse_test\": rmse_test,\n",
    "    \"rmspe_train\": metric_train,\n",
    "    \"rmspe_test\": metric_test,\n",
    "}\n",
    "df_metrics = pd.DataFrame.from_dict(d_metrics, orient=\"index\").T\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-recorder",
   "metadata": {},
   "source": [
    "The goal was to develop a forecast sales with an [Root Mean Squared Percentage Error](https://link.springer.com/article/10.1007/s10342-014-0793-7) (RMSPE) of 10% or less. The RMSPE on the test set here is approximately 14%, which is larger and so future work should aim to achieve an improvement in the ML model developed/data used here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-pharmacy",
   "metadata": {},
   "source": [
    "### Observed and Predicted Sales, as a function of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-narrative",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "np.expm1(df_pred[[\"Sales\", \"pred\"]]).head(25_000).plot(ax=ax)\n",
    "ax.grid()\n",
    "ax.set_xlabel(None)\n",
    "ax.legend(ncol=2, bbox_to_anchor=(0.8, 1.1125), loc=\"upper left\", frameon=False)\n",
    "ax.set_title(\"Obs vs Pred\", loc=\"left\", fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-rogers",
   "metadata": {},
   "source": [
    "<a id=\"distribution-of-residuals\"></a>\n",
    "\n",
    "### 13.2. [Distribution of Residuals](#distribution-of-residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-tuner",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "df_pred[\"res\"].plot.hist(edgecolor=\"white\", color=\"blue\", ax=ax)\n",
    "ax.set_ylabel(None)\n",
    "ax.grid(color=\"lightgrey\", alpha=1)\n",
    "ax.set_title(\"Distribution of Residuals\", fontweight=\"bold\", loc=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-beatles",
   "metadata": {},
   "source": [
    "<a id=\"prediction-error---observed-vs-predicted\"></a>\n",
    "\n",
    "### 13.3. [Prediction Error - Observed vs Predicted](#prediction-error---observed-vs-predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-calendar",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(df_pred[[\"pred_Raw\"]], df_pred[\"Sales_Raw\"])\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "df_pred[[\"Sales_Raw\", \"pred_Raw\"]].plot.scatter(\n",
    "    x=\"pred_Raw\", y=\"Sales_Raw\", color=\"white\", edgecolors=\"b\", s=40, ax=ax\n",
    ")\n",
    "ax.plot(\n",
    "    df_pred[\"pred_Raw\"].to_numpy(),\n",
    "    reg.predict(df_pred[[\"pred_Raw\"]]),\n",
    "    label=f\"best fit (R2={reg.score(df_pred[['pred_Raw']], df_pred['Sales_Raw']):.2f})\",\n",
    "    color=\"darkred\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(None)\n",
    "ax.grid(color=\"lightgrey\", alpha=0.75)\n",
    "ax.set_title(\n",
    "    f\"Observed vs Predicted (RMSPE={metric_test:.2f})\", fontweight=\"bold\", loc=\"left\"\n",
    ")\n",
    "ax.axline([0, 0], [1, 1], label=\"identity\", color=\"darkorange\", linestyle=\"--\")\n",
    "ax.legend(frameon=False)\n",
    "ax.set_xlim((0, None))\n",
    "ax.set_ylim((0, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-aquarium",
   "metadata": {},
   "source": [
    "<a id=\"residuals-vs-observed\"></a>\n",
    "\n",
    "### 13.4. [Residuals vs Observed](#residuals-vs-observed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-upset",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "df_pred.loc[df_pred[\"res\"] > -10_000][[\"Sales_Raw\", \"res\"]].plot.scatter(\n",
    "    x=\"Sales_Raw\", y=\"res\", color=\"white\", edgecolors=\"b\", s=40, ax=ax\n",
    ")\n",
    "ax.set_xlabel(None)\n",
    "ax.set_ylabel(None)\n",
    "ax.axline([0, 0], [1, 0], color=\"black\", linestyle=\"--\")\n",
    "ax.grid(color=\"lightgrey\", alpha=0.75)\n",
    "ax.set_title(\"Residual vs Observed\", fontweight=\"bold\", loc=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-revolution",
   "metadata": {},
   "source": [
    "<a id=\"quantile-quantile-plot-for-residuals\"></a>\n",
    "\n",
    "### 13.5. [Quantile-Quantile Plot for Residuals](#quantile-quantile-plot-for-residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-chemistry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_qq(\n",
    "    df_pred[\"res\"],\n",
    "    fig_size=(6, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-canon",
   "metadata": {},
   "source": [
    "<a id=\"export-trained-ml-model\"></a>\n",
    "\n",
    "## 14. [Export trained ML model](#export-trained-ml-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-rebate",
   "metadata": {},
   "source": [
    "<a id=\"train-ml-model-with-all-available-training-data-and-no-early-stopping\"></a>\n",
    "\n",
    "### 14.1. [Train ML model with all available training data and no early-stopping](#train-ml-model-with-all-available-training-data-and-no-early-stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-gossip",
   "metadata": {},
   "source": [
    "Next, get the actual number of rounds where early stopping was triggered and use this for training the model on all the available training data - at this time, early stopping rounds will no longer be required since we know the exact round (`num_boost_round_best` below) when the reduction in loss has stopped and so we can hard-code this into the model under the `num_boost_round` hyper-parameter of the (Learning API's) [`.train()` method](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-space",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_boost_round_best = model_exp002.best_iteration + 1\n",
    "print(num_boost_round_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-mobile",
   "metadata": {},
   "source": [
    "Next, train the model on all the available training data. This means re-running all the processing steps in this notebook (listed below, excluding those that have a line drawn through them since no testing data willl be available and no early stopping is required), using the full training dataset\n",
    "- Extract Features and Target from full training data\n",
    "- Handle missing values in full training data - Use Median of full training data\n",
    "- Encode Categorical Features\n",
    "- Scaling Numerical Features\n",
    "- Transform Target\n",
    "- Train with Best HyperParameters and Export to disk in preparation for inference\n",
    "  - Train with best hyper-parameters\n",
    "  - ~~XGBoost Training Curves~~\n",
    "  - ~~Make predictions~~\n",
    "  - Export trained ML model to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-timeline",
   "metadata": {},
   "source": [
    "<span style='color:red'><b>To be done</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-august",
   "metadata": {},
   "source": [
    "<a id=\"export-ml-model-to-file\"></a>\n",
    "\n",
    "### 14.2. [Export ML model to file](#export-ml-model-to-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-agency",
   "metadata": {},
   "source": [
    "Finally, [export the model to a `pickle` file](https://cloud.google.com/ai-platform/prediction/docs/getting-started-scikit-xgboost#xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model_exp002, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-handy",
   "metadata": {},
   "source": [
    "<a id=\"notes\"></a>\n",
    "\n",
    "## 15. [Notes](#notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-calibration",
   "metadata": {},
   "source": [
    "1.  As mentioned [earlier](#hyper-parameter-tuning-with-k-fold-cross-validation) (see the comment starting with `# Select a random sample for early stopping`), the custom cross-validation splitter generates training, validation and testing splits. Since hyper-parameter tuning is done using a (5,000 observation) sample of the training split (the first 2,500 being used for training and the next 2,500 for validation), this means that the validation split produced by the cross-validation splitter is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-crowd",
   "metadata": {},
   "source": [
    "<a id=\"future-work\"></a>\n",
    "\n",
    "## 16. [Future Work](#future-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-senior",
   "metadata": {},
   "source": [
    "1.  <span style='color:red'><b>(IMPORTANT) Scale features (standardization or normalization)</b></span> during the following steps\n",
    "    -   [final model training](#scaling-numerical-features)\n",
    "    -   [cross-validation](#hyper-parameter-tuning-with-k-fold-cross-validation)\n",
    "2.  Is the regression target (`y`) [stationary](https://en.wikipedia.org/wiki/Stationary_process) (in a timeseries sense) for each store?\n",
    "3.  As mentioned [earlier](#hyper-parameter-tuning-with-k-fold-cross-validation), fix the custom cross-validation splitter so that the validation and test split lengths (within the same fold) are equal in length.\n",
    "4.  Perform hyper-parameter tuning using full cross-validation data splits\n",
    "    -   currently, this is only done with a sample (as shown in the [`score()` helper function](#hyper-parameter-tuning-with-k-fold-cross-validation))\n",
    "5.  Examine prediction errors (eg. over- or under-predictions), grouped by\n",
    "    -   store\n",
    "    -   state\n",
    "    -   school holiday\n",
    "    -   state holiday\n",
    "    -   other categorical features\n",
    "6.  Explore XGBoost Hyper-Parameter settings during [cross-validation](#hyper-parameter-tuning-with-k-fold-cross-validation)\n",
    "    -   increase number of estimators (`num_boosting_rounds`)\n",
    "    -   modify `early_stopping_rounds`\n",
    "    -   other XGBoost hyper-parameters (`eta`, `max_depth`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-telling",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-windsor",
   "metadata": {},
   "source": [
    "<span style=\"float:left;\">\n",
    "    <a href=\"./1_data_clean_feat_eng.ipynb\"><< 1 - Data Cleaning and Feature Engineering</a>\n",
    "</span>\n",
    "\n",
    "<span style=\"float:right;\">\n",
    "    &#169; 2021 | <a href=\"https://github.com/edesz/streetcar-delays\">@edesz</a> (MIT)\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
