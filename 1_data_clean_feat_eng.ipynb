{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metropolitan-motor",
   "metadata": {},
   "source": [
    "# Data Cleaning, Merging and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-scope",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-citizen",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from isoweek import Week\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-wiring",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%aimport src.data_custom_transformers\n",
    "import src.data_custom_transformers as ct\n",
    "\n",
    "%aimport src.data_helpers\n",
    "from src.data_helpers import left_merge_dfs\n",
    "\n",
    "%aimport src.features_helpers\n",
    "from src.features_helpers import add_datepart, get_elapsed\n",
    "\n",
    "%aimport src.test_helpers\n",
    "from src.test_helpers import test_train_test_target_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-transformation",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0.  [About](#about)\n",
    "1.  [User Inputs](#user-inputs)\n",
    "2.  [Load Data](#load-data)\n",
    "3.  [Data Cleaning](#data-cleaning)\n",
    "4.  [Feature Extraction](#feature-extraction)\n",
    "5.  [Merging Data Sources](#merging-data-sources)\n",
    "6.  [Handle Missing values](#handle-missing-values)\n",
    "7.  [Feature Engineering](#feature-engineering)\n",
    "    -   7.1. [Elapsed Time - Competitors](#elapsed-time---competitors)\n",
    "    -   7.2. [Elapsed Time - Promo2](#elapsed-time---promo2)\n",
    "    -   7.3. [Add time before and after special events](#add-time-before-and-after-special-events)\n",
    "    -   7.4. [Weekly Rolling Average, of number of special-event days, by store](#weekly-rolling-average,-of-number-of-special-event-days,-by-store)\n",
    "8.  [Dropping columns from `LEFT_JOIN`](#dropping-columns-from-`left-join`)\n",
    "9.  [Export merged data](#export-merged-data)\n",
    "10.  [Ideas for Exploratory Data Analysis](#ideas-for-exploratory-data-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-brazilian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-17T14:26:32.381936Z",
     "iopub.status.busy": "2021-03-17T14:26:32.381752Z",
     "iopub.status.idle": "2021-03-17T14:26:32.387662Z",
     "shell.execute_reply": "2021-03-17T14:26:32.387012Z",
     "shell.execute_reply.started": "2021-03-17T14:26:32.381889Z"
    }
   },
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-seminar",
   "metadata": {},
   "source": [
    "In this notebook, the raw store-wise sales data (the primary data source) will be processed, and merged wtih the secondary datasets (weather, state names, etc.). Features will be engineered from the merged data. This will be done separately for the training and testing sales datasets. Each of the merged datasets will then be exported to a [parquet](https://databricks.com/glossary/what-is-parquet) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-sponsorship",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-inclusion",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    \"store\",\n",
    "    \"state_names\",\n",
    "    \"store_states\",\n",
    "    \"test\",\n",
    "    \"train\",\n",
    "    \"weather\",\n",
    "]\n",
    "train_date_max = \"2015-07-25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-expression",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data_path = os.path.join(PROJ_ROOT_DIR, \"data\", \"raw\")\n",
    "processed_data_path = os.path.join(PROJ_ROOT_DIR, \"data\", \"processed\")\n",
    "d = {tn: os.path.join(raw_data_path, \"rossmann\", f\"{tn}.csv\") for tn in dataset_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-norway",
   "metadata": {},
   "source": [
    "<a id=\"load-data\"></a>\n",
    "\n",
    "## 2. [Load Data](#load-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-outside",
   "metadata": {},
   "source": [
    "Each dataset will be loaded into a separate `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-pulse",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "(\n",
    "    store,\n",
    "    state_names,\n",
    "    store_states,\n",
    "    test,\n",
    "    train,\n",
    "    weather,\n",
    ") = [\n",
    "    pd.read_csv(d[dataset_name], low_memory=False)\n",
    "    for dataset_name in dataset_names\n",
    "]\n",
    "print(len(train),len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-craps",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(store.head())\n",
    "display(store.dtypes.to_frame().T)\n",
    "display(state_names.head())\n",
    "display(state_names.dtypes.to_frame().T)\n",
    "display(store_states.head())\n",
    "display(store_states.dtypes.to_frame().T)\n",
    "display(train.head())\n",
    "display(train.dtypes.to_frame().T)\n",
    "display(weather.head())\n",
    "display(weather.dtypes.to_frame().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-elephant",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The primary datasets available are `train` and `test`, and secondary datasets provided are `store`, `store_states`, `state_names` and `weather`.\n",
    "2. `sotre` provides store details\n",
    "3. `state_names` provides the full state name and the state name acronym\n",
    "4. `store_states` provides the state name acronym for each store (store number)\n",
    "5. `train` and `test` provide store-wise data for a single date, including `Sales` (only in the `train`ing data), store number, state name (full name of the state), number of customers, promotion, school holiday, state holiday, date and day of week (Mon-Fri), and whether the store was open on that date\n",
    "6. `weather` provides the weather conditions by date for each state (full state name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-kelly",
   "metadata": {},
   "source": [
    "<a id=\"data-cleaning\"></a>\n",
    "\n",
    "## 3. [Data Cleaning](#data-cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-cameroon",
   "metadata": {},
   "source": [
    "The `StateHoliday` column in the (primary) `train` and `test` datasets is a string with a value not equal to `1` if the date on a single row corresponds to a state holiday. This string will be converted to a boolean value (`True`, indicating the date is a state holiday, if the value is not equal to `0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "str2bool_pipe = Pipeline(\n",
    "    [\n",
    "        (\"str2bool\", ct.DFStr2Bool(\"StateHoliday\", \"0\", \"ne\")),\n",
    "    ]\n",
    ")\n",
    "train = str2bool_pipe.fit_transform(train)\n",
    "test = str2bool_pipe.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-impossible",
   "metadata": {},
   "source": [
    "<a id=\"feature-extraction\"></a>\n",
    "\n",
    "## 4. [Feature Extraction](#feature-extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-auckland",
   "metadata": {},
   "source": [
    "`datetime` attributes will be extracted from the `Date` column of the `weather`, `train` and `test` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-integral",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "add_datepart_pipe = Pipeline(\n",
    "    [\n",
    "        (\"adddatepart\", ct.DFAddDatePart(\"Date\", False, False, False)),\n",
    "    ]\n",
    ")\n",
    "weather = add_datepart_pipe.fit_transform(weather)\n",
    "train = add_datepart_pipe.fit_transform(train)\n",
    "test = add_datepart_pipe.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-lawsuit",
   "metadata": {},
   "source": [
    "**Note**\n",
    "1. In order to assist in merging these datasets, the `Date` column will be retained after extracting the `datetime` attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-bookmark",
   "metadata": {},
   "source": [
    "<a id=\"merging-data-sources\"></a>\n",
    "\n",
    "## 5. [Merging Data Sources](#merging-data-sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-printing",
   "metadata": {},
   "source": [
    "All the secondary datasets (`store`, `store_states`, `state_names`, `weather`) will now be separately merged with the (primary) `train`ing and `test`ing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-coalition",
   "metadata": {},
   "source": [
    "First, merge <font color='green'>weather</font> data with the <font color='orange'>state</font> dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-cutting",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "weather = weather.merge(state_names, left_on=\"file\", right_on=\"StateName\", how=\"left\", suffixes=(\"\", '_y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-hydrogen",
   "metadata": {},
   "source": [
    "Next, merge <font color='red'>store</font> data with the name of the <font color='blue'>state</font> in which the store is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-trash",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "store = left_merge_dfs(store, store_states, \"Store\")\n",
    "len(store[store.State.isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-salad",
   "metadata": {},
   "source": [
    "Next, separately merge the raw training and testing <font color='purple'>sales</font> data with the merged <font color='red'>store</font>-<font color='orange'>state</font> data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "joined = left_merge_dfs(train, store, \"Store\")\n",
    "joined_test = left_merge_dfs(test, store, \"Store\")\n",
    "len(joined[joined.StoreType.isnull()]), len(joined_test[joined_test.StoreType.isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-crown",
   "metadata": {},
   "source": [
    "Next, merge the merged <font color='purple'>sales</font>-<font color='red'>store</font>-<font color='orange'>state</font> data with the <font color='green'>weather</font> data\n",
    "- the `Date` column will be part of this merge, since observations in each dataset involved in this merge are listed by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-riverside",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "joined = left_merge_dfs(joined, weather, [\"State\",\"Date\"])\n",
    "joined_test = left_merge_dfs(joined_test, weather, [\"State\",\"Date\"])\n",
    "len(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-health",
   "metadata": {},
   "source": [
    "We'll do a quick sanity check to verify that the target variable (`Sales`) is not in the merged testing data and that the merged testing data contains one less column than the merged training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_test_target_var(joined, joined_test, \"Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-annotation",
   "metadata": {},
   "source": [
    "Since all shared columns between dataset-pairs in each merge were not used to perform the merge, this will result in suffixes (`_y`) appended to these column names. This results in duplicated columns where one column name in each pair of such shared columns ends in `_y`. So, next, one of the columns that were duplicated after the merges (column name ending in `_y`) will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "drop_cols_by_suffix_pipe = Pipeline(\n",
    "    [\n",
    "        (\"dropsuffix\", ct.DFDropColsBySuffix(\"_y\")),\n",
    "    ]\n",
    ")\n",
    "joined = drop_cols_by_suffix_pipe.fit_transform(joined)\n",
    "joined_test = drop_cols_by_suffix_pipe.fit_transform(joined_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-london",
   "metadata": {},
   "source": [
    "<a id=\"handle-missing-values\"></a>\n",
    "\n",
    "## 6. [Handle Missing values](#handle-missing-values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-resistance",
   "metadata": {},
   "source": [
    "Next, missing data in special events columns will be handled. A placeholder value, *that does not appear elsewhere in the corresponding column in the* **training** <font color='purple'>sales</font> data, will be used to fill in missing values in each column in both the merged training and testing datasets.\n",
    "\n",
    "Also, the `datatype` for each of these columns will be set to an integer (since the filled in placeholder value is of integer `dtype`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-musical",
   "metadata": {},
   "source": [
    "Unique values occurring in each of these columns with missing values are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-romance",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "na_cols = [\n",
    "    \"CompetitionOpenSinceYear\",\n",
    "    \"CompetitionOpenSinceMonth\",\n",
    "    \"Promo2SinceYear\",\n",
    "    \"Promo2SinceWeek\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-olive",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in na_cols:\n",
    "    display(joined[c].dropna().astype(int).value_counts().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-standard",
   "metadata": {},
   "source": [
    "Missing values are now filled in and the `datatype` is changed to `int`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fillna_placeholder_pipe = Pipeline(\n",
    "    [\n",
    "        (\"nancompetitionopensinceyear\", ct.DFFillNaPlaceHolder(\"CompetitionOpenSinceYear\", 1900)),\n",
    "        (\"competitionopensinceyearint\", ct.DFDtypeChanger(\"CompetitionOpenSinceYear\", np.int32)),        \n",
    "        (\"nancompetitionopensincemonth\", ct.DFFillNaPlaceHolder(\"CompetitionOpenSinceMonth\", 1)),\n",
    "        (\"competitionopensincemonthint\", ct.DFDtypeChanger(\"CompetitionOpenSinceMonth\", np.int32)),        \n",
    "        (\"nanpromo2sinceyear\", ct.DFFillNaPlaceHolder(\"Promo2SinceYear\", 1900)),\n",
    "        (\"promo2sinceyearint\", ct.DFDtypeChanger(\"Promo2SinceYear\", np.int32)),        \n",
    "        (\"nanpromo2sinceweek\", ct.DFFillNaPlaceHolder(\"Promo2SinceWeek\", 1)),\n",
    "        (\"promo2sinceweekint\", ct.DFDtypeChanger(\"Promo2SinceWeek\", np.int32)),        \n",
    "    ]\n",
    ")\n",
    "joined = fillna_placeholder_pipe.fit_transform(joined)\n",
    "joined_test = fillna_placeholder_pipe.fit_transform(joined_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-correlation",
   "metadata": {},
   "source": [
    "<a id=\"feature-engineering\"></a>\n",
    "\n",
    "## 7. [Feature Engineering](#feature-engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-increase",
   "metadata": {},
   "source": [
    "<a id=\"elapsed-time---competitors\"></a>\n",
    "\n",
    "### 7.1. [Elapsed Time - Competitors](#elapsed-time---competitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-vehicle",
   "metadata": {},
   "source": [
    "We'll extract features \"CompetitionOpenSince\" and \"CompetitionDaysOpen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-republic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "for df in (joined, joined_test):\n",
    "    df[\"CompetitionOpenSince\"] = pd.to_datetime(\n",
    "        dict(\n",
    "            year=df[\"CompetitionOpenSinceYear\"],\n",
    "            month=df[\"CompetitionOpenSinceMonth\"],\n",
    "            day=15,\n",
    "        )\n",
    "    )\n",
    "    df[\"CompetitionDaysOpen\"] = df[\"Date\"].subtract(df[\"CompetitionOpenSince\"]).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-decimal",
   "metadata": {},
   "source": [
    "We'll replace some erroneous / outlying data\n",
    "-   replace the number of days for which competitors have been open by zero, if they have been open for less than 0 days\n",
    "-   replace the year since which competitors have been open by zero, if they have been open since prior to the year 1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for df in (joined,joined_test):\n",
    "    df.loc[df[\"CompetitionDaysOpen\"] < 0, \"CompetitionDaysOpen\"] = 0\n",
    "    df.loc[df[\"CompetitionOpenSinceYear\"] < 1990, \"CompetitionDaysOpen\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-extraction",
   "metadata": {},
   "source": [
    "We add the \"CompetitionMonthsOpen\" field, limiting the maximum to 2 years to limit the number of unique categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-massage",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for df in (joined,joined_test):\n",
    "    # Convert days to months\n",
    "    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"] // 30\n",
    "    # limit the max to 2 years (24 months) to limit number of unique categories\n",
    "    df.loc[df.CompetitionMonthsOpen > 24, \"CompetitionMonthsOpen\"] = 24\n",
    "print(joined.CompetitionMonthsOpen.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-assault",
   "metadata": {},
   "source": [
    "<a id=\"elapsed-time---promo2\"></a>\n",
    "\n",
    "### 7.2. [Elapsed Time - Promo2](#elapsed-time---promo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-watson",
   "metadata": {},
   "source": [
    "We'll similarly extract features \"Promo2Since\" and \"Promo2Days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-north",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for df in (joined, joined_test):\n",
    "    df[\"Promo2Since\"] = pd.to_datetime(\n",
    "        df.apply(lambda x: Week(x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1)\n",
    "    )\n",
    "    df[\"Promo2Days\"] = df.Date.subtract(df[\"Promo2Since\"]).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-inspector",
   "metadata": {},
   "source": [
    "We'll replace some erroneous / outlying data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for df in (joined,joined_test):\n",
    "    df.loc[df[\"Promo2Days\"] < 0, \"Promo2Days\"] = 0\n",
    "    df.loc[df[\"Promo2SinceYear\"] < 1990, \"Promo2Days\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-reproduction",
   "metadata": {},
   "source": [
    "We add the \"Promo2Weeks\" field, limiting the maximum to 6 months to (as before) limit the number of unique categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for df in (joined, joined_test):\n",
    "    # Convert days to weeks\n",
    "    df[\"Promo2Weeks\"] = df[\"Promo2Days\"] // 7\n",
    "    # limit ourselves to 6 months to limit number of unique categories\n",
    "    df.loc[df[\"Promo2Weeks\"] < 0, \"Promo2Weeks\"] = 0\n",
    "    df.loc[df[\"Promo2Weeks\"] > 25, \"Promo2Weeks\"] = 25\n",
    "    print(df[\"Promo2Weeks\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-brass",
   "metadata": {},
   "source": [
    "<a id=\"add-time-before-and-after-special-events\"></a>\n",
    "\n",
    "### 7.3. [Add time before and after special events](#add-time-before-and-after-special-events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-billy",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1.  This will be done across a subset of features\n",
    "2.  This requires `DataFrame`s to be sorted by the relevant field and then by `Date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Date\", \"Store\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-hollow",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = train[columns].append(test[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-greek",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fld = 'SchoolHoliday'\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "get_elapsed(df, fld, 'After')\n",
    "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
    "get_elapsed(df, fld, 'Before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-bishop",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fld = 'StateHoliday'\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "get_elapsed(df, fld, 'After')\n",
    "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
    "get_elapsed(df, fld, 'Before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-murray",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fld = 'Promo'\n",
    "df = df.sort_values(['Store', 'Date'])\n",
    "get_elapsed(df, fld, 'After')\n",
    "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
    "get_elapsed(df, fld, 'Before')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-compact",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-corner",
   "metadata": {},
   "source": [
    "Then set null values from elapsed field calculations to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-conditioning",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "columns = ['SchoolHoliday', 'StateHoliday', 'Promo']\n",
    "for o in ['Before', 'After']:\n",
    "    for p in columns:\n",
    "        a = o+p\n",
    "        df[a] = df[a].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-portable",
   "metadata": {},
   "source": [
    "<a id=\"weekly-rolling-average,-of-number-of-special-event-days,-by-store\"></a>\n",
    "\n",
    "### 7.4. [Weekly Rolling Average, of number of special-event days, by store](#weekly-rolling-average,-of-number-of-special-event-days,-by-store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-separate",
   "metadata": {},
   "source": [
    "Next, we'll calculate the number of special events by store, on a rolling weekly basis. This will give the rolling number of weekly\n",
    "- school holidays\n",
    "- state holidays\n",
    "- promotions\n",
    "\n",
    "per store.\n",
    "\n",
    "This means for tomorrow's observation in the sales data, the rolling total (number) of special-event days over the past week (7 days) will be computed and inserted as a value in a new column. This will be done separately for each store in the sales dataset, and also separately for each of the special-event columns (`SchoolHoliday`, `StateHoliday`, `Promo`).\n",
    "\n",
    "This first requires a sort by date, to ensure the observations are in chronological order, and then count the number of weekly occurrences of special events grouped by store. This gives a backward looking rolling total.i.e. looking into the past (7 days). If the sorting by date is done in descending order (to get later dates to appear first, and have observations appear in reverse chronological order) then this gives a forward-looking rolling total. Both will be used here to engineer rolling statistic features for each of the special-event columns.\n",
    "\n",
    "The steps followed are\n",
    "- calculate backward looking rolling statistic (number of special events)\n",
    "- calculate forward looking rolling statistic\n",
    "- remove `Store` level from the index of the forward- and backward-looking store-wise aggregations\n",
    "- merge rolling statistic (total) with source data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-associate",
   "metadata": {},
   "source": [
    "The relevant columns (special events) as well as the `Store` column are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-private",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_store_numbers = [1]\n",
    "df_few_stores = df[df[\"Store\"].isin(few_store_numbers)][\n",
    "    [\"Store\"] + columns\n",
    "].sort_index()\n",
    "display(df_few_stores.head(2 * 7))\n",
    "display(df_few_stores.tail(2 * 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-course",
   "metadata": {},
   "source": [
    "A preview (showing the first and last 14 days) of the rolling total is shown below for store number 1 in the backward looking direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-advocacy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_few_stores_roll_stats = df_few_stores.sort_index().groupby(\"Store\")[columns].rolling(7, min_periods=1).sum().astype(int)\n",
    "df_few_stores_roll_stats[\"weekday\"] = df_few_stores_roll_stats.index.get_level_values(1).day_name()\n",
    "display(df_few_stores_roll_stats.head(2*7))\n",
    "display(df_few_stores_roll_stats.tail(2*7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-punch",
   "metadata": {},
   "source": [
    "and similarly, for the same store, in the forward looking direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-container",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_few_stores_roll_stats_reverse_chron = df_few_stores.sort_index(ascending=False).groupby(\"Store\")[columns].rolling(7, min_periods=1).sum().astype(int)\n",
    "df_few_stores_roll_stats_reverse_chron[\"weekday\"] = df_few_stores_roll_stats_reverse_chron.index.get_level_values(1).day_name()\n",
    "display(df_few_stores_roll_stats_reverse_chron.head(2*7))\n",
    "display(df_few_stores_roll_stats_reverse_chron.tail(2*7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-table",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "roll_stats_pipe = Pipeline(\n",
    "    [\n",
    "        (\"rollstats\", ct.DFMultiDirMultiColRollingStat(\"Store\", columns)),\n",
    "    ]\n",
    ")\n",
    "df = roll_stats_pipe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-belarus",
   "metadata": {},
   "source": [
    "Convert the `Date` column to `datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_datetime_pipe = Pipeline(\n",
    "    [\n",
    "        (\"todatetime\", ct.DFToDatetime(\"Date\")),\n",
    "    ]\n",
    ")\n",
    "df = to_datetime_pipe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-structure",
   "metadata": {},
   "source": [
    "Merge the fully merged dataset from [above](#merging-data-sources) with this merged rolling statistic-source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "joined = left_merge_dfs(joined, df, ['Store', 'Date'])\n",
    "joined = joined[joined.Sales!=0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "joined_test = left_merge_dfs(joined_test, df, ['Store', 'Date']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-optimization",
   "metadata": {},
   "source": [
    "<a id=\"dropping-columns-from-`left-join`\"></a>\n",
    "\n",
    "## 8. [Dropping columns from `LEFT_JOIN`](#dropping-columns-from-`left-join`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-tonight",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(\n",
    "    joined.columns[\n",
    "        joined.columns.str.contains(\n",
    "            \"|\".join([\"Promo\", \"StateHoliday\", \"SchoolHoliday\"])\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-gathering",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "joined = joined.drop(joined.columns[joined.columns.str.endswith(\"_y\")].tolist(), axis=1)\n",
    "joined_test = joined_test.drop(joined_test.columns[joined_test.columns.str.endswith(\"_y\")].tolist(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-planner",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(joined.shape)\n",
    "print(joined_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-tenant",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-selling",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(joined.head().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-joseph",
   "metadata": {},
   "source": [
    "<a id=\"export-merged-data\"></a>\n",
    "\n",
    "## 9. [Export merged data](#export-merged-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-compound",
   "metadata": {},
   "source": [
    "We'll define a variable with the path to the parquet file (to be saved) in the raw data folder created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_parquet_filepath = os.path.join(\n",
    "    processed_data_path, \"cleaned_train\" + \"_\" + timestr + \".parquet\"\n",
    ")\n",
    "test_parquet_filepath = os.path.join(\n",
    "    processed_data_path, \"cleaned_test\" + \"_\" + timestr + \".parquet\"\n",
    ")\n",
    "print(train_parquet_filepath)\n",
    "print(test_parquet_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-village",
   "metadata": {},
   "source": [
    "We'll now save the merged datasets to a separate `parquet` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-decrease",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for df, split_type, fpath in zip(\n",
    "    (joined, joined_test),\n",
    "    (\"train\", \"test\"),\n",
    "    (train_parquet_filepath, test_parquet_filepath),\n",
    "):\n",
    "    try:\n",
    "        print(f\"Saving {split_type}ing data to {fpath + '.gzip'}\", end=\"...\")\n",
    "        df.to_parquet(\n",
    "            fpath + \".gzip\",\n",
    "            engine=\"auto\",\n",
    "            index=False,\n",
    "            compression=\"gzip\",\n",
    "        )\n",
    "        print(\"done.\")\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-friendship",
   "metadata": {},
   "source": [
    "<a id=\"ideas-for-exploratory-data-analysis\"></a>\n",
    "\n",
    "## 10. [Ideas for Exploratory Data Analysis](#ideas-for-exploratory-data-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-coral",
   "metadata": {},
   "source": [
    "Based on data cleaning and feature engineering, the following is a preliminary list of relationships in the data to be examined via exploratory data analysis\n",
    "-   on a per store basis, explore rolling total of weekly special event days versus weekly sales\n",
    "-   sales per state\n",
    "-   sales on special event days regular regular business days\n",
    "-   sales on school or state holidays versus regular business days\n",
    "-   sales by various datetime attributes\n",
    "-   on a per store basis, sales as a function of time\n",
    "-   sales per store\n",
    "-   sales versus weather events [`Mean_Wind_SpeedKm_h`, `Precipitationmm`,`CloudCover` (maybe categorical), `Events` (categorical)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-quarterly",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-alcohol",
   "metadata": {},
   "source": [
    "<span style=\"float:left;\">\n",
    "    &#169; 2021 | <a href=\"https://github.com/edesz/streetcar-delays\">@edesz</a> (MIT)\n",
    "</span>\n",
    "    \n",
    "<span style=\"float:right;\">\n",
    "    <a href=\"./2_xgboost_trials.ipynb\">2 - Regression Trials with XGBoost >></a>\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
